[["index.html", "Picturing Flights with R UNDER CONSTRUCTION Exploring open data with open software Chapter 1 How to use this book 1.1 Before you start 1.2 Ways to Use the Material 1.3 Structure of the book 1.4 Acknowledgements 1.5 What’s gone wrong?", " Picturing Flights with R UNDER CONSTRUCTION Exploring open data with open software David Marsh 2023-02-09 Chapter 1 How to use this book — UNDER CONSTRUCTION! — There’s a growing range of freely-available data on aviation, covering flight patterns, airport locations, aircraft emissions, delays and whole lot more. You would be hard pushed to find anything similar for walking, road, rail or maritime, making aviation an exceptionally open mode of transport to explore, research and analyse. However, it can be a problem knowing where to start. Visualising the data that you have found is often a good way to begin to understand what you might be able to do with it. The open-source software R provides a powerful set of functions for analysis and visualisation. RStudio makes it easy to use R well. They’re both free to download. As with the data, it can be hard to find your way around R, because sometimes it seems that there are a dozen ways to do anything. This book introduces both the aviation data and R, explaining through simple, but real examples. It aims to help you build a small but useful set of tools and data sources. In the first chapters, we take some first steps in R, using aviation examples and with an emphasis on making graphs of the data. In later chapters, we move on to data analysis and more complex topics in mapping and text analysis. 1.1 Before you start You need to have RStudio and R installed on your machine. This book is available as html here and can be downloaded complete with data from here (TBD). Air transport and air traffic management, like any domain, have their own jargon. This isn’t a training book for air transport, but it will explain some key ideas along the way. There are plenty of useful ‘cheat sheets’ for R, I’ll introduce my favourites as we go through. 1.2 Ways to Use the Material The book aims to support you using it in a number of different ways: Training. Work through chapters in order, or pick out specific chapters if you need a refresher on a topic (an R topic, or an aviation topic). Read the explanations, grab the code and run it, and test your understanding by answering questions [in square brackets in the text] and doing exercises. Reference. Use the book search (top left of this page) to find some how-to material directly. Snippets. The book builds up snippets of usable code (TBD - how complete?) for you to cut-and-paste. But it is never going to replace the huge amount of excellent information, especially in stackoverflow. A key principle is that “Google is your friend”: even if I prefer DuckDuckGo for many purposes, a Google search is often more productive for R. Whether it’s for how to do something, or how to respond to an error, usually someone has already suffered, and some kind person has answered. Google it. 1.3 Structure of the book Chapters 1 and 2 should get you up and running in RStudio. Chapters 3 to 5 start with how to look at data, using a data viewer and with some initial graphs. If you’re mostly using existing code that creates simple charts, written by someone else, then these chapters should help you find your way around, and start to adapt the code for your own needs. Chapters 6 (and TBD) do more data loading and wrangling, covering the basics of loops and functions more…TBD Chapter 8 introduces the map functions in R, including where to find airspace structure data, and how to plot both great circle routes and actual route flown. Chapter 9 covers pulling data out of strings (parsing) using ‘regular expressions’, with NOTAMs as the application. 1.4 Acknowledgements This book has partly been written whilst at Eurocontrol and partly during a visiting research fellowship at Manchester Metropolitan University. Thanks to colleagues in both organisations for their ideas and patience. Any views expressed are entirely unofficial. 1.5 What’s gone wrong? Learning from mistakes is essential, so each chapter has a “what’s gone wrong?” section near the end, discussing some typical potholes that we all fall into from time to time. Look there, especially if you don’t see the results from the code in the book that you were expecting. We start the book with one classic pitfall: R is case sensitive, which can be quite a culture shock if you’ve been brought up on other systems. This matters for filenames as well as code. When in doubt, check whether you’ve got the right mix of upper- and lower-case letters! Or avoid the problem by using the little helpers in R Studio, such as the ‘tab’ key for completing filenames. "],["start.html", "Chapter 2 Getting started with the basic tools of R 2.1 Orientation in RStudio 2.2 First project 2.3 First code 2.4 First packages 2.5 File types 2.6 What’s gone wrong? 2.7 Test yourself", " Chapter 2 Getting started with the basic tools of R We said in chapter 1 that you need first to install R and RStudio. These are separate pieces of software: R does all the statistical and graphics stuff, while RStudio provides the graphical user interface. In this chapter we get up and running in RStudio, and see some very basic R code. 2.1 Orientation in RStudio Here we take a brief look around the RStudio interface. Use ‘RStudio/Help’ to get more detailed help. The RStudio interface can be customised almost beyond recognition. We’ll use a mix of styles in the book so that you don’t get too fixed on seeing only one, but it’s probably helpful to your colleagues not to re-order the main four panes, otherwise they’ll find looking over your shoulder or screen-sharing a disorienting experience. A basic MS Windows RStudio, with work on the go, looks something like this. RStudio snapshot The main panes of the screen are: Top left: source code, shown as a number of tabs, one for each file; Bottom left: the ‘console’, which is a scratchpad for entering code, and where log output is usually shown (and some other tabs which we don’t need here); Top right: the ‘environment’ and ‘history’ tabs are of main interest. Environment is where you can explore all the data you’ve created. History is useful for re-doing something, particularly as you can search for code. Bottom right: This has several important tabs Files: for exploring files within a project, can be quicker than using the windows explorer; Plots: is where plots will appear (usually); Packages: is for checking which packages are installed, or active (see section 2.4); Help: all the details of the functions that you will need - this is usually quicker to use than googling a function (though the same help files also come up when you google, from various providers around the web). The buttons that appear around the panes are context-sensitive: they will change according to the type of file that you have open. There are some hot-keys for moving rapidly around the panes: most often, I use ctrl-1 to go to the source code, ctrl-2 for the console. You can then guess the others, or find them by trial and error. Recent versions of RStudio have a tutorial (tab in top right pane) if you need more detail. Try out the console (bottom left, ctrl-2). Try typing 3 + 4 * 2 there (and press ‘enter’). You should see “[1] 11”, meaning that the first “[1]” (and in this case only) element of your answer is 11. The spaces in that calculation are optional, but recommended for ease of reading. [Exercise: Try it without the spaces.] If you’d like to see an answer with more than one element, type letters into the console. This is a built-in constant. Check the ‘help’ for ‘letters’ to see some others. 2.2 First project The console is good for quick, throw-away calculations. But it’s a bit like treating R as a pocket calculator. Instead, we want to save R code in files. While you can work with ‘bare’ files of R source code, ‘script files’ in the jargon or ‘scripts’, we think it’s tidier to use ‘projects’, for two main reasons: you can work with several shorter script files (and other types of source file that we won’t be looking at) together, which makes it easier to organise and navigate; and a project automatically remembers which directory it’s working in, so you can manage data input and output and graphic output more neatly. Create yourself a new project using ‘File/New Project’, selecting the options new project, give it the name ‘justlearning’ and browse to put it in your personal R directory. The sequence should look a little like this, with variation coming from where you create the new project subdirectory. That final part depends on your system and filing habits. new project, step 2 new project, step 3 In that final step, if you are working on a Windows machine you will see a slightly different path to your ‘user directory’ under ‘Create project as a subdirectory of:’. It’s rare that you need to change this, anyway. The project will open without a code panel (which would have been top left), because you have no code yet. It looks like this. In this book we’ll assume that projects always keep data in the data directory, and save graphs to the graphs directory. You can create these quickly in your new project by copying the code from here (quick-copy icon appears top right in the code block when you move the mouse over it) and pasting it into the console (and press ‘enter’). # good to have these in every project dir.create(&quot;data&quot;) dir.create(&quot;graphs&quot;) If for some reason the directories already exist, don’t worry, you’ll just get a warning. The line beginning with ‘#’ is a comment which is ignored. You can also create these directories manually using ‘New Folder’ in the files tab (ctrl-5), but then make sure both folder names are in lower case! You could even use your operating system file explorer - these are just ordinary directories (‘folders’), there’s nothing R-special about them. When you quit RStudio it will save any data and open files in your project. So you can re-open and continue from where you left off. 2.3 First code Now that you’ve got a blank project, add a new blank R script file using File/New File/R Script, or the ‘file plus’ icon top left. It appears in the source code pane, top left. Immediately save it; call it ‘chapter2’ for example. The name isn’t critical here, but avoid spaces and punctuation. By default, it will be saved to the top level in the project. This is fine for many projects, though in some cases we might choose to organise code differently. It’s good practice to comment as you go along, using #. At the top of your new script put some comments saying who you are, the date, and what it is for. Then add a comment for each chunk of code, or for any lines you think might be difficult for you to understand when you come back. I forget where I read this idea, but think of comments as ‘notes for a future you’. Type this code into your new ‘chapter2’ script, either manually or copy paste. 3 + 4 # I know pi pi 1:50 cos(pi/3) # angle is in radians Unlike in the console, in a script file code doesn’t get executed as you type. You have to run it. Usually you’re either: stepping through code, running a bit at a time, in which case ctrl-enter (cmd-enter on the Mac) is easy to use: it runs the code where the cursor currently sits most of the time intelligently selecting other lines that need to run at the same time, and then moves to the next line of code, jumping over comments that are preceded by #; running all of the code, which you can do by pressing the ‘source’ button (or select all and ctrl-enter). The output appears in the console as a running log. It should look like this: 2.4 First packages The R language is expanding continually as people publish new packages for it. A ‘package’ provides a collection of functions, often some data and sometimes some new data types. If you’re starting in R, and just aiming to find your way around in and use code, then mostly what you need to know is how to load packages (and what that means), and a little about the more common ones. That’s what we cover in this section. 2.4.1 Package basics Some packages are already bundled in the basic installation of ‘R’, such as base which provides, as the name suggests, many of the most basic functions. But there are thousands of other packages, coming from: CRAN, the “Comprehensive R Archive Network” is the authoritative collection of packages. There are also various ‘mirror’ (official copy) sites hosted elsewhere, such as at Ghent University. Packages in CRAN have been through a degree of quality control, and are preferred to the less official sources. Github and other public repositories. Even Eurocontrol shares some there, such as himach and the PRC dashboard. Home-made. Making packages is out of the scope of the book, but if you’re inside Eurocontrol, you may want to load the statfor package created by Sebastien that builds some access to Eurocontrol datawarehouses as well as nice formatting. There are two1 steps to using a package, and sometimes these get confused: Installation. You type install.packages(\"package_name\") and R finds the files for that package and saves them on your machine or on a network drive. So the files are available. You’ve done the shopping and the food is in the kitchen cupboards. Attaching or Loading. You have to make a package available for use for your session, usually with library( ). You’ve pulled the ingredients for your recipe out of the cupboard and they’re on the kitchen table. If there’s a difference between ‘attaching’ and ‘loading’ then it doesn’t matter here. A few packages are automatically loaded at the start of the session. In the packages pane, a package is listed if it is installed and ticked if it’s loaded. In the screenshot, just the base package is loaded of the ones listed in this screenshot. Clicking on the package name takes you to the documentation. There’s a manual for all functions together, as a pdf, but it’s usually easier to use the help pane to get the same material, and perhaps copy examples from the end of each help entry. More importantly here there are links to vignettes, and often now to websites with more info. Vignettes are not little stickers, but essential how-to guides mixing text and code. Often this can provide a skeleton end-to-end structure showing how to use functions, and that you can copy and adapt to your own need. You can load a package by ticking the box in the packages pane, but normally it’s done in code, as in this example. library(lubridate) # lots of date-related functions. ## Loading required package: timechange ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union Two packages can define a function with the same name. The CRAN repository of packages performs quality checks, but overlap between packages is not something CRAN controls. Think of CRAN as an excellent library rather than an ‘Académie française’ for R that controls which words are allowed into the language2. When there is overlap in packages, both defining a function with the same name, say, then on loading the second one ‘masks’ the first (you should see this above). Sometimes, as an alternative to loading the full package, you might see in code a ‘two colon’ usage, such as base::union. This is very common inside packages (it’s recommended), and it’s a way to insist that the first package version of the function should be used. As well as function with the same name and different results, there are often many different functions that you could use to achieve the same result, eg base::paste0() and stringr::str_c() both concatenate strings. # both concatenate two strings, inserting no separator between them base::paste0(&quot;On Ilkley Moo&quot;, &quot;r.&quot;) ## [1] &quot;On Ilkley Moor.&quot; stringr::str_c(&quot;On Ilkley Moo&quot;, &quot;r.&quot;) ## [1] &quot;On Ilkley Moor.&quot; In this example, notice that the strings to be concatenated are written in quotes. It’s recommended to use double “, rather than single ‘. And while we’re on such conventions, the space after the comma in the parameter list is optional, but recommended for ease of reading. In fact you’ll see that when there are lots of parameters or long ones, we tend to move to a new line, also for ease of reading. Meanwhile, there should be no space between the function name and the’(’. There’s an art in R to doing the most with the minimum number of packages, since it takes time to find your way around the functions in a package. That’s like the satisfaction of making the recipe from things you already have in the cupboard. But sometimes, you just don’t want to make the flaky pastry yourself. There are times when you’re looking at a complex task and should be thinking ‘surely someone has already tackled this in R?’. A little googling will often find you most of the pieces already in place in a new package, or one you have but had forgotten about. You develop a personal ‘dialect’ of R, from the packages that you choose to use most often. We’ll discuss one of the most common dialects, the tidyverse, in more detail in the next section. Since it helps if you and colleagues share a dialect, everyone adopting the tidyverse is a good start. 2.4.2 Tidyverse The tidyverse is our chosen dialect, in the sense that in most cases we’ll use the functions and data structures, and way of organising, that go with this collection of packages. There’s a lot of excellent documentation already available, so we will explain some basics here, and introduce other elements as we need them for flight data examples. The tidy in ‘tidyverse’ refers to a tidy data structure: a table with each variable in its own column and each observation on one row. While we often find flight data with years displayed across the table, and countries down the side, this is not ‘tidy’. Table 2.1: An untidy table. Country Flights2019 Flights2020 France 1.3 1.5 Germany 5.2 6.0 Table 2.1: A tidy table. Country Year Flights France 2019 1.3 Germany 2020 5.2 France 2019 1.5 Germany 2020 6.0 The main data structure used by the tidyverse is the dataframe, although increasingly the tidyverse prefers the ‘tibble’ tbl, which is a specific sort of dataframe. We won’t worry about the differences here. For most ‘quick pieces of code’, the easiest is to start your R script with a library(tidyverse) to load all the parts of the tidyverse. If you were writing a package, that wouldn’t be very efficient, because there’s quite a lot of it. Sometimes you’ll see individual parts of the tidyverse loaded including: ggplot2: lots of plotting functions (see next chapters) dplyr: for manipulating and processing data tidyr: for tidying data, such as pivot-table like actions, or splitting columns. And there are packages which are on the outskirts of the ‘tidyverse’ which get announced when you first load. The full list (at the time of writing) is as follows. ## ── Attaching packages ──────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ─────────────── tidyverse_conflicts() ── ## ✖ lubridate::as.difftime() masks base::as.difftime() ## ✖ lubridate::date() masks base::date() ## ✖ dplyr::filter() masks stats::filter() ## ✖ lubridate::intersect() masks base::intersect() ## ✖ dplyr::lag() masks stats::lag() ## ✖ lubridate::setdiff() masks base::setdiff() ## ✖ lubridate::union() masks base::union() ## [1] &quot;broom&quot; &quot;cli&quot; &quot;crayon&quot; &quot;dbplyr&quot; ## [5] &quot;dplyr&quot; &quot;dtplyr&quot; &quot;forcats&quot; &quot;ggplot2&quot; ## [9] &quot;googledrive&quot; &quot;googlesheets4&quot; &quot;haven&quot; &quot;hms&quot; ## [13] &quot;httr&quot; &quot;jsonlite&quot; &quot;lubridate&quot; &quot;magrittr&quot; ## [17] &quot;modelr&quot; &quot;pillar&quot; &quot;purrr&quot; &quot;readr&quot; ## [21] &quot;readxl&quot; &quot;reprex&quot; &quot;rlang&quot; &quot;rstudioapi&quot; ## [25] &quot;rvest&quot; &quot;stringr&quot; &quot;tibble&quot; &quot;tidyr&quot; ## [29] &quot;xml2&quot; &quot;tidyverse&quot; These include lubridate and stringr which we’ve already mentioned. We’ll see a lot more tidyverse usage in later chapters, when we get to grips with data wrangling. Just to whet your appetite, here we show two quick examples. Try the code out, but don’t worry if it’s a little cryptic at this stage, we will explain the parts in more detail later. Start with the untidy dataframe shown above. Use pivot_longer to make it tidy, all in one go selecting a number of columns (those that start with ‘Flights’) and extracting the ‘year’ from this, then pivoting to the tidy form. Then we show a pairing of group_by and summarise to produce some annual totals. This second step example doesn’t save its result to a dataframe but prints it immediately to the log. untidy &lt;- data.frame(Country = c(&quot;France&quot;, &quot;Germany&quot;), Flights2019 = c(1.3, 5.2), Flights2020= c(1.5, 6.0)) # tidy the data tidier &lt;- pivot_longer(untidy, # pivot the two columns starting with &#39;Flights&#39; cols = starts_with(&quot;Flights&quot;), # put the column names in a column called &#39;year&#39; names_to= &quot;year&quot;, # ignore the &#39;Flights&#39; bit of the name, and treat as integer names_pattern = &quot;Flights(.*)&quot;, names_transform = list(year = as.integer), # put the column values in a column called &#39;flights&#39; values_to = &quot;flights&quot;) print(tidier) ## # A tibble: 4 × 3 ## Country year flights ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 France 2019 1.3 ## 2 France 2020 1.5 ## 3 Germany 2019 5.2 ## 4 Germany 2020 6 # using groups - just print the result tidier %&gt;% group_by(year) %&gt;% summarise(total_flights = sum(flights)) ## # A tibble: 2 × 2 ## year total_flights ## &lt;int&gt; &lt;dbl&gt; ## 1 2019 6.5 ## 2 2020 7.5 Don’t worry if this demo seems complicated, or to come out of nowhere - we’ll take it more slowly in Chapter TBD. But it illustrates the principle we mentioned earlier: for many operations (like tidying a table), you’re not the first to need to do this, so there’s probably a neat way to do it in R. You’ll see another short bit of tidyverse at the beginning of the next chapter. 2.5 File types You will see lots of different file types (ie file extensions), in the files pane. The main ones to remember are: .R, .Rmd: both contain R code, though .Rmd is actually ‘r markdown’ which is a mix of code and text; .RProj: contains an R ‘project’ - if you see one of these in the directory, this is the one to open - everything else works from there; .rda, .RDS: are different types of data file. R works easily also with .csv and .xls(x). Try the book search to find places where these are discussed. 2.6 What’s gone wrong? If you see a window like this, then you have started R rather than RStudio. That is a GUI and you can use it to execute R code, but you’ll find RStudio easier for all but the quickest snippets of code. If it keeps on happening you will find it helpful to associate R files (.R, .RMD and others) with RStudio rather than R. If an RStudio project takes a long time to start, perhaps you had a very large file in your environment when you last closed the project. You might think about removing large files that you don’t need with rm(...) before you close the project. If you get errors saying a certain package is only compatible with version xx and higher (of R), and you think you’ve recently updated, are you sure you updated R rather than RStudio? (Even if this is a task for IT, perhaps they misunderstood and updated the wrong one?) The RStudio version is found from the top menu ‘RStudio/About R Studio’, the R version is seen when you first start up (see the image just above), or is printed if you type version in the console. If you’re searching in ‘help’ and a function isn’t appearing, for example in the drop-down as you type, it is probably because it comes from a package that isn’t loaded. You can finish typing, and the system will search and may find it, but also some other less good matches. Or if you know the package name, you can type ?ggplot2::geom_line for example, in the console to go straight there. 2.7 Test yourself 2.7.1 Questions Where will you (usually) find the help on functions? Which of these provides a graphical user interface (GUI): R, RStudio? Which ctrl-key combination takes you to the environment pane? What does an .rda file contain? What’s the difference between typing into a script file and into the console? What is a ‘vignette’? (teaser) What does 3 + 1:3 give? Does R only store pi to 5 decimal places, as shown earlier? [Hint: options(digits = 22) and then pi.] 2.7.2 Answers Bottom right in RStudio. Both, though you’ll nearly always want to use RStudio. CTRL-8 One or more R datasets With the console, code is executed as each line is completed. An extended entry in the documentation showing how to put the functions of the packages together. 4 5 6 No.(You’ll find the default value for digits in the help for options.) Alternative answer, if you’re into trigonometry, is no because otherwise cos(pi) wouldn’t give -1. actually, there’s a third, but that’s too much detail for here↩︎ Actually, CRAN does enforce some quality control but that’s more about how packages work than what packages there might be, with which functions.↩︎ "],["firstLook.html", "Chapter 3 First look at data and CO2 emissions 3.1 Looking at data: CO2 Data 3.2 Extracting variables 3.3 Extracting a few values 3.4 CO2 Scatter plot 3.5 What’s gone wrong? 3.6 Test yourself", " Chapter 3 First look at data and CO2 emissions It’s hard to get far in an analysis without first looking at the data to ask questions such as: What variables are there? Do I know what they all mean? What time period does it cover? Which countries, or airports etc, are included? In this chapter, we introduce some of the ways to take a quick look at your data. We also introduce some data on CO2 emissions per European State from aviation. In this chapter, you’ll be introduced to: read_xlsx(), &lt;-, summary, str, environment pane, View(), unique, head, c(), [], ggplot() Re-open your justlearning project (File:Recent Projects, or ‘justLearning.Rproj’). Create a new script file (File:New File:R script), to copy and paste the examples into, and save it as ‘chapter3’. Although the project re-opens your datasets and scripts, it starts in a new R session. That means that you have to re-load the package(s) that you need, as in this code. So you might find it easier to have this at the start of your ‘chapter3’ script. library(tidyverse) 3.1 Looking at data: CO2 Data We use public data on national CO2 emissions from aviation available on the EUROCONTROL/AIU website. We choose this, apart from the interest in the data themselves, because it’s a small set so quick to download, and it’s already tidy (each variable in one column). To isolate this book from changes in the original file, we use a version that we’ve saved to github. # download the file to the data folder, the &#39;mode&#39; parameter is needed on Windows machines co2_url &lt;- &quot;https://github.com/david6marsh/flights_in_R/raw/main/data/CO2_emissions_by_state.xlsx&quot; # if you want the original, which might have been updated, use this instead # co2_url &lt;- &quot;https://ansperformance.eu/download/xls/CO2_emissions_by_state.xlsx&quot; download.file(co2_url, &quot;data/CO2_emissions.xlsx&quot;, mode = &quot;wb&quot;) # load from the DATA worksheet - case sensitive! aviation_co2 &lt;- readxl::read_excel(&quot;data/CO2_emissions.xlsx&quot;, sheet = &quot;DATA&quot;) We’ve already seen the function(parameter) way to call a function in the cos(pi/3) example. Now we have something &lt;- function(parameter, parameter). This is a peculiarity of R that you just need to get used to. Think of it as saying: create something in the environment (without saying what it is just yet), then fill it (&lt;-) with the results of the function(). While you might occasionally have used a function in Excel, for example, in R basically everything you do is call functions. Now that the Excel data are downloaded, in your R script comment out the line download.files(..., because we don’t need to keep downloading if you happen to re-run the code. Comment means putting a # in front of it. Pressing shift-ctrl-C (shift-cmd-C on a Mac) does the same, and also works over many lines at once. Notice that R doesn’t mind if a function is split over several lines. If there were frequent updates, maybe you would repeatedly download (so not comment out). See (TBD) for an example. [See the Excel, now in your project data folder, for disclaimer and details.] International conventions mean that CO2 emissions are measured from flights departing airports in a state. read_xlsx automatically selects the first row as variable names. One tool in the ‘explore your data’ toolbox is summary. summary(aviation_co2) ## YEAR MONTH STATE_NAME STATE_CODE ## Min. :2010 Min. : 1.000 Length:5694 Length:5694 ## 1st Qu.:2012 1st Qu.: 3.250 Class :character Class :character ## Median :2015 Median : 6.000 Mode :character Mode :character ## Mean :2015 Mean : 6.498 ## 3rd Qu.:2018 3rd Qu.: 9.000 ## Max. :2020 Max. :12.000 ## CO2_QTY_TONNES TF ## Min. : 0 Min. : 1 ## 1st Qu.: 15665 1st Qu.: 1364 ## Median : 84312 Median : 4605 ## Mean : 336396 Mean : 16376 ## 3rd Qu.: 279711 3rd Qu.: 17073 ## Max. :3541111 Max. :120473 The summary function is fairly basic, but it gives a quick feel for what’s in the data. Often more helpful with numeric than character variables, but also useful for spotting if there are missing values NA. So we can see there are no missing values here. In this dataset we have: YEAR: An integer, not a date, but read_xlsx reads this as a real number (ie potentially having decimals). MONTH: An integer, giving the month in the year. Again, this has been assumed to be real rather than integer. STATE_NAME, STATE_CODE: A long name and the 2-letter ‘country code’ derived from the ICAO 4-letter communication address, of which the less said, the better. CO2_QTY_TONNES: Total annual CO2 emissions, in (metric) tonnes. TF: Total flights. This is departing flights. Flights through a state’s airspace that do not land are not counted, nor are arrivals from outside the state. A domestic flight is counted once, as a departure. You’ll notice a few names in there which aren’t states, such as ‘Canaries’ which is counted separately from Spain. These measurements add up cleanly (this isn’t always true in flights data), so you can get the full ‘Spain’ by adding the two. There are also some States with a ’*’ next to their names, which means there’s a footnote elsewhere in the spreadsheet. We want to keep things simple, so we will use a little data wrangling to aggregate to yearly totals for each country, then save the result as an R dataset, rather than an Excel file. These are some quite common step in data wrangling: Drop some variables. Here, just select everything except than STATE_CODE. (Not is a very thin !, so easily overlooked.)3 Summarise. group_by the relevant variables, then summarise within those groups. Here we use the summarise_at variant of summarise, which allows us quickly to apply a function sum to multiple variables. Drop some rows. There are few families and business unaffected by COVID-19. Aviation is of course no exception, and the data from 2020 are really an outlier for this reason. To keep things simple here, we omit 2020, using a filter that says ‘keep only the values before 2020’. The group_by isn’t essential for the filter, but I think it’s probably faster as written here, since the grouping process has already found all of the year 2020 values. Often you want to keep your groups, but here we don’t need to, so we ungroup at the end. Then we save the data into the data subdirectory of the project. There will be more examples of this sort of data manipulation in Chapter 4, with more explanation of what’s happening. And a lot more when we turn to data wrangling in chapters 6 and 7. annual_co2 &lt;- aviation_co2 %&gt;% select(!STATE_CODE) %&gt;% group_by(YEAR, STATE_NAME) %&gt;% summarise_at(vars(CO2_QTY_TONNES, TF), sum) %&gt;% filter(YEAR &lt; 2020) %&gt;% ungroup() save(annual_co2, file = &quot;data/annual_co2.rda&quot;) There are three other important ways to explore the data. Firstly in the environment pane (top right, CTRL-8), where you can click the first line, with name on, to see a summary. You get much the same thing in the console by typing str(annual_co2) where str is for ‘structure’. You should see something like this. We can see that there are three numeric variables (num) and two character variables (chr). All five variables have the same number of observations (437). In a tibble or dataframe the columns are always the same length. The second way to explore the structure, because this is a tibble, is just to type its name in the console. [Try it] This is useful but just bear in mind that for some data structures, this might fill up your console with a lot of output. Save it for when you’re sure you’ve a tibble. You can check by typing class(my_thing) into the console to see if my_thing is a tibble (shown as tbl). The third way gives a window to explore every observation. Click on the dataset name next to the blue arrow or type View(annual_co2) in the console (sorry about the upper case ‘V’, R is like that) and you get a tabular data explorer, which allows you to sort and filter. You should see something like this. [Try out the sorting and filtering in the view window. Filter to show only the Netherlands, and sort by total flights.] 3.2 Extracting variables To answer more questions about the data there are some more tools to summarise the values that it takes. We saw summary() works for numeric values, but what about discrete ones? There are several ways to pull one variable out of your data. We’ll use the $ notation, partly because there’s a reminder of this in the environment tab. [Where is this ‘reminder’?] # pull out all values in the column state_vbl &lt;- annual_co2$STATE_NAME states &lt;- unique(state_vbl) Look in the environment pane, state_vbl is listed under ‘Values’. It’s a (column) vector, one of the simple data types in R which is why it’s listed under ‘Values’ and not under ‘Data’. ‘Data’ is for more complex data structures, such as dataframes and tibbles. You’ve just pulled a column out of annual_co2 so not surprising that it has the same number of rows as annual_co2. And the first values are all ALBANIA, or were when this book was compiled. Really we want to know how many different states there are, and which ones. unique does what it says, and we’ve saved these as states; a variable name which to me implies ‘unique states’. You can tell how many there are from the environment pane, or you could use length(states). [How many are there?] To inspect all of these values you can just type states into the console, a good way to check the spelling of some, perhaps. [Try this. Is it ‘Canaries’ or ‘Canarias’?] The order in which the elements are shown is as in the original data, there’s no re-ordering unless you ask for it. If you’ve worked with SAS PROC SQL or other languages, it might come as a relief to hear that, in R, the order of rows stays where it’s put until you say otherwise; none of this need to sort before every operation. We’ll see some ways to handle ‘top’ values later (section 4.3). So in this case, even if the states are in alphabetical order, that’s just because the original Excel file was. 3.3 Extracting a few values We’ve just seen how to pull a variable out of a tibble, as a vector. How do we extract one or more values out of the vector that we created? The states are quite a lot to show in the console. Sometimes you just need to see a quick sample, eg to check if they’re in title case, or if they’re codes or names. head() is useful for showing you the first few (6 by default). head(states) ## [1] &quot;ALBANIA&quot; &quot;ARMENIA&quot; &quot;AUSTRIA&quot; ## [4] &quot;BELGIUM&quot; &quot;BOSNIA AND HERZEGOVINA&quot; &quot;BULGARIA&quot; If you want to pull out a single value, or a few of them, again there are multiple ways to do this, but the simplest is this. We show here two ways to select with a vector of numbers: creating a consecutive sequence of numbers (1:3); and creating a vector with an arbitrary selection (c(1, 5, 10)). states[1] ## [1] &quot;ALBANIA&quot; states[3] ## [1] &quot;AUSTRIA&quot; states[1:3] ## [1] &quot;ALBANIA&quot; &quot;ARMENIA&quot; &quot;AUSTRIA&quot; states[c(1, 5, 10)] ## [1] &quot;ALBANIA&quot; &quot;BOSNIA AND HERZEGOVINA&quot; &quot;CZECHIA&quot; We’ll deal with subsetting the whole dataset, rather than just a vector extracted from it, in the next chapter. 3.4 CO2 Scatter plot It’s hard to beat a graph as a way to explore data. So we end the chapter exploring a simple graph. The tidyverse way of doing this is to use ggplot. 3.4.1 First draft In the most basic scatter plot we have the following components, joined with a +. This + is peculiar to ggplot; another lovable quirk of R. Learn it, but get used to the idea that you’ll forget and use other conjunctions by mistake at times. In the simplest code we have: ggplot: with parameters the data to use, and an ‘aesthetic’ aes; aes: gives the x and y first, and here also says choose colour based on year; geom_point: says to plot points with these data, ie a scatter plot. ggplot(annual_co2, aes(TF, CO2_QTY_TONNES, colour = YEAR)) + geom_point() Even this simple example illustrates that: a parameter to a function (here ggplot) can be another function call (here aes()); we can specify parameters by position of appearance in the list (x and y are first and second for aes) so we don’t have to name them, or we can specify a parameter by name (colour), or a mix of both (as long as the position ones come first!); geom_point takes its aesthetics by default from the ggplot statement. We’ll see later that you can add to or override this default (eg section 4.5). There’s a (very) rough correlation along a diagonal line, but it would be interesting to know which states are above the line (more CO2 per flight) and which below. And is the change gradual, or is there much variability? 3.4.2 Improve the titles Let’s at least label the axes so that someone else can see quickly what has been plotted. We can transform variables on the fly (using the rule that a parameter can be a function call, here to the function /), so let’s convert both axes to millions (1e6 means 1*10^6, so millions with less risk of getting the number of 0s wrong). The label on the legend is meaningful, but to avoid the block capitals we can change that too within the labs() statement. It’s a ‘colour’ legend (that’s what is in the aes call), so you need to refer to ‘colour’ in the labs(). ggplot(annual_co2, aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR)) + geom_point() + labs(x = &quot;Departing Flights (millions)&quot;, y = &quot;CO2 (million tonnes)&quot;, colour = &quot;Year&quot;, title = &quot;Emissions per state&quot;) 3.4.3 and with clustering by state It’s tempting to read the graph as having a number of small clusters, each with flights and CO2 increasing with time, and assume that each of these corresponds to a single state. It would be nice to use the graph to see if that’s true. There are too many countries to give each its own shape (we’ll see shapes used more effectively in section TBD), but we can easily add a line to join the points for each state. [Working from geom_point, how do you think you would add a line?] We need both to add a line, which follows the pattern of geom_point, and group by state. That’s done in the same way that we coloured by year, in the aesthetics. There are several ways to plot a line. The most obvious one, having seen geom_point previously, is geom_line. However, this joins the points in x-axis order. We want data order, so that’s geom_path. Out of a sense of neatness, we also add a subscript to CO2. The code bquote(~CO[2]~\" (million tonnes)\") took some googling and is cryptic, but it works! ggplot(annual_co2, aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions per state&quot;) The graph isn’t ready for a presentation yet, but a story is already emerging. The lines do a pretty good job of grouping the years for each state together. We see a graph with 6 busy states, 3 of which are fairly linear, so a relatively fixed CO2/flight. Three others are more variable from year to year. Then there are two states, with flights in the 0.25-0.5 million flights range and that diverge from the main trend line, respectively higher and lower. The remaining, smaller states rather overlap in this graph. We’ll find out how to pick out and label states in the next chapter. 3.5 What’s gone wrong? Why do I get a message saying something like “could not find function …”? When you re-open RStudio, remember that it goes some of the way to restarting where you left off (opening files and loading datasets for you), but it opens in a new R ‘session’. That means that only the base packages are available. That’s why your R scripts usually start with a bunch of library() statements - you have to start by running these to get the packages back for this session. It pays to looks closely at the graph and try to explain what you see. In fact, it was only once I’d tidied the colours up that I noticed that they were not in order along the line. I had used geom_line (join in x-axis order) in place of geom_path (join in data order). So story-telling can help debugging too. Also, it pays to read the help file, even if you think you know how the function works. We’ll see more about grouping in later chapters. It’s quite common that I get errors in some tidyverse data wrangling, because the dataset is grouped, and I had forgotten that. So grouping is powerful and quick, but R remembers your groups longer than you do! 3.6 Test yourself 3.6.1 Questions Use head to view the first 10 states. Using the help file for head, how would you display the last 6 states? Which state names are followed by ’*’? Print the distinct state codes to the console. Print the 3rd, 23rd and 33rd state names to the console. What does &lt;- do? In ggplot what does + do? 3.6.2 Answers head(states, 10) head is documented alongside tail: use tail(states) (In my version of the data), typing states into the console, and by eye I see 4 states with a ’*’. We’ll see other ways to do this, later. unique(aviation_co2$STATE_CODE) states[c(3, 23, 33)] It puts the results of whatever is on the right-hand side into the object on the left. It connects parts of the definition of a graph together. In fact, this step is here just to illustrate the selection step. It’s not needed, as we’ll see in the data wrangling chapters, or you could test by removing it.↩︎ "],["filter.html", "Chapter 4 Filtering a dataset and refining the CO2 graph 4.1 Sequences of functions 4.2 Filtering datasets and logical tests 4.3 Selecting the busiest states 4.4 CO2 graph for the top states 4.5 Labelling the CO2 graph 4.6 What does the graph say about CO2? 4.7 What’s gone wrong? 4.8 Exercises", " Chapter 4 Filtering a dataset and refining the CO2 graph In this chapter we improve the CO2 emissions graph, en route learning how to filter observations from a dataset, to add new variables, and to use the pipe operator %&gt;%. In this chapter, you’ll be introduced to: filter(), ==, %in%, mutate(), slice_max(), %&gt;%, geom_text_repel(), scales_colour_... On re-opening your justlearning project, you should still have the annual_co2 dataset in your environment, since RStudio saves these data and reloads on restart. If not, you should find that annual_co2.rda is saved in the data folder, and you can load it using load(\"data/annual_co2.rda\"). One R speciality here is that you don’t need to assign the result with a annual_co2 &lt;- ...; in fact the single file that we saved could have contained a number of datasets, and they’re all reloaded with their original dataset names.4 If you have closed RStudio and re-opened it, then you have a new session and need to reload packages, in this case library(tidyverse). Make yourself a new R script file for ‘chapter4’ starting with this. 4.1 Sequences of functions It’s time for a bit more syntax. We saw already that you can combine functions by making one the parameter to another. Or to put it another way, wrapping one around the other. As you combine more functions, this soon becomes hard to read, and you have to rely on the editor to help you spot whether you’ve enough brackets closed at the right point. For that reason we use a different syntax, the ‘pipe’ operator %&gt;% introduced by the magrittr package and adopted by the tidyverse. If you learn only one control-key combination in RStudio, do learn shift-ctrl-M (also shift-command-M on Mac). This must be the fastest way to type ‘%&gt;%’! [Try it in your console.] With a new line after each %&gt;% and appropriate indenting (RStudio helps with that), you get code that looks like this. This says, fill a with what you get from dataset b after applying function fun1, then function fun2. # which is clearer # with a pipe? a &lt;- b %&gt;% fun1(p1) %&gt;% fun2(p2, p3) # or without? a &lt;- fun2(fun1(b, p1), p2, p3) The pipe syntax works because fun1 actually has a parameter list that starts with the dataset to which it should be applied fun1(data, p1,...). So b %&gt;% fun1(p1) is just another way of writing fun1(b, p1). Or, the other way around, you can use %&gt;% whenever you have a function whose first parameter is the dataset to be operated on. It turns out that this is true for very many of them, and the tidyverse is designed that way. [Does the code to summarise by year that you saw in section 3.1 make more sense now that you know about %&gt;%?] 4.2 Filtering datasets and logical tests As is so often the case, in R there are several of ways to select rows from a dataframe or tibble. We’ll focus on the filter(data, test) function. For this we need to know how to construct a logical test. There are three parts to this: the logic, the test functions and what’s being tested. Logic is mostly given by &amp;, ! and |5 for ‘and’, ‘not’ and ‘or’, grouped with round brackets in the normal way. The test functions are almost as you might expect: &gt;, &lt;. However, in R you need to use == not = to test for equality. I suspect this creates the most common typo in R code! Check the documentation of dplyr::filter for a more complete list of test functions: in particular look out for %in%, which we will use shortly. One really nice touch in filter() is in the third part: what’s being tested. One of the trickiest bits of learning R is knowing how, within a function, to refer to one or more variables of the dataset. In filter() you can just use the name of the variable; so no quotes needed around the name, and the code assumes it is a variable from the data parameter, so no need to use annual_co2$.... In this example, we don’t push the result into a dataset (no a &lt;-), so it gets printed out directly. annual_co2 %&gt;% filter(YEAR == 2018 &amp; STATE_NAME %in% c(&quot;CZECHIA&quot;, &quot;ALBANIA&quot;)) ## # A tibble: 2 × 4 ## YEAR STATE_NAME CO2_QTY_TONNES TF ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018 ALBANIA 143871. 12744 ## 2 2018 CZECHIA 1381672. 90679 Note that: the order of the rows is as in the original dataset, not at all influenced by the order of the naming of the states in the test YEAR and STATE_NAME in the filter function are so-called ‘bare’ strings (ie without quotes) that name variables, and are shorthand for annual_co2$YEAR etc but “CZECHIA” is a value of a variable so needs to be a string in quotes and the test is case-sensitive. [Try changing it to “Czechia”.] 4.3 Selecting the busiest states In the previous chapter we selected the first-named states with head(). Now we do something more useful: selecting the top states by flights, using slice_max(). We need to define this a bit more clearly. ‘Top’ could mean in a particular year, or over the whole period (where flights have decreased as well as increased). We choose to mean ‘top by flights in 2019’. This is partly out of habit (top in the latest year is often the meaning) and partly because we need to introduce fewer new bits of R to implement it. The code is like this. A final novelty is that we use pull() which, as its help-file says, does the same as $ (learned in the last chapter) but looks nicer in pipes. [Where can you look at top_states to check that there are 8 values?] top_states &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # top in year 2019 slice_max(TF, n = 8) %&gt;% # top 8 pull(STATE_NAME) slice_max() is a good example of how R changes with time. New versions of packages introduce minor or major changes. Sometimes a function is superseded (left to rot), other times it may be deprecated (you have some time to switch to a new version before it’s removed). The function top_n, which you might see lying around in legacy code, has been superseded by slice_max(). [Check out the documentation of top_n for some of the reasons.] These changes mean that just updating to the latest version of the package is not always the best idea, because you might have to spend some time checking for changes. It also means that, when searching on the web for hints, snippets and answers, you need to look at the date of the answer. ggplot in particular has changed quite a bit, so answers more than 5 years old or so might not be that helpful. 4.4 CO2 graph for the top states With top_states in place we can easily plot the data for the busiest states. We update the title, and add a footnote (caption) to explain what’s going on. We could have created a new dataset, eg top_co2 &lt;- annual_co2 %&gt;% filter(STATE_NAME %in% top_states), and then used this in the ggplot. But we only plan to use this filter once, so to avoid cluttering the environment with datasets, we filter ‘on the fly’, within the ggplot statement. In this case, this is a matter of personal preference. If the datasets were a lot larger, and we intended analysing and transforming just the top states in further graphs, the decision might be different. ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) One advantage of filtering on the fly is that we can change on the fly. [Change the filter to the states not in the top 8. It takes one key press. Re-run the graph. Update the titles too.] As an analyst, I really want to know which country is which. With just 8 states shown, maybe I can use symbols? At one level it’s quick to do. Just replace group = with shape =. [Try this.] ggplot complains that it doesn’t really like using more than 6 symbols, because it becomes hard for the reader to jump between legend and graph. We could roll with it and learn how to extend the palette, but is there another solution? We can separate states by colours. Again, quite quick: replace colour = YEAR, group = STATE_NAME with colour = STATE_NAME. [Try this.] The line means we can work out the ordering of the years easily, so losing the year isn’t a big issue here. But eight colours is also a lot to tell apart. Even without colour blindness, you might think they’re clear but when projected onto a screen or printed or on a tiny phone screen, perhaps not. 4.5 Labelling the CO2 graph It’s relatively easy to follow a slightly different route: adding state names directly to the graph. This is done with geom_text added to the ggplot. We only want to label the point in 2019 rather than all years, so we create a new variable which is empty except in rows for the year 2019. mutate(a = ...) is the function for adding a variable ‘a’ to the dataset6. if_else() is how we give a value for only some years. As with the filter() function, we can refer to other variables in annual_co2 without inverted commas. This time, we amend the annual_co2 dataset itself, because we want to be able to use this in several places, not just in a single graph. The syntax a &lt;- a %&gt;% ... follows the same pattern as you saw earlier, but you’re overwriting the original dataset. The geom_text() inherits all of the aesthetics from the ggplot function, so it already knows where to find x and y coordinates, and what colour to use. We still need to tell geom_text() where to find the labels. This is also an aesthetic. annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2019, STATE_NAME, &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + geom_text(aes(label = state_label)) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) This is close, but not quite good enough. The text is centred on the 2019 point, and this creates some ugly overlaps. There are lots of options in geom_text() to adjust the position, and you’ll find lots of examples on the web. So we could spend time adjusting the positions. But this is a first example of the rule: ‘surely someone has already come across this problem?’. Someone has indeed spent time to come up with good ways to deconflict and position labels on graphs. The package is called ggrepel, which you might need to install with install.packages(\"ggrepel\"), and it provides a ‘drop in’ replacement for geom_text naturally called geom_text_repel. To avoid using library(\"ggrepel\") when we’re just using one function from the package on one occasion, we use the double-colon syntax in the code (see 2.4.1). The defaults for this function work pretty well in this particular case. But there are a couple of things I’d like to fix: the block capitals and the year legend. Title case would be nicer, so we convert the STATE_NAME using the stringr package function str_to_title(). stringr is already loaded as part of the tidyverse, and since most of its functions begin ‘str_’ it’s quite easy to start searching in the help pane for the right one [Try this.]. In this case state_label already exists and we overwrite it. The other thing to improve is the year scale, which shows with meaningless decimals. We use a quick-ish fix, rather than the tidiest-possible solution. Scales, whether the axes or colours, are controlled by ggplot functions starting scales_ in this case scales_colour_steps() gets us a scale that shows the individual years. This is a ‘dirty’ solution in the sense that, if the data for 2020 get included, you might need to tweak the code; but then, we’ve hard-coded 2019 in a number of places, so this is dirty elsewhere. We’ll see cleaner options later (for example in section 5.3.1). annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2019, str_to_title(STATE_NAME), &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + ggrepel::geom_text_repel(aes(label = state_label)) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) 4.6 What does the graph say about CO2? Longer-haul flights use heavier aircraft and therefore the proportion of long-haul flights in a national mix is a major influence on CO2 per flight. For example, the Netherlands has more CO2 per flight than Norway: Norway has a significant domestic (so short-range) market, which the Netherlands does not, being instead a major long-haul hub. These two states have been relatively stable. The UK also has a proportionally larger long-haul market. And a decline in its domestic market has led to quite a rapid increase in CO2 per flight in recent years. So, the graph helps to build a story: though we needed some supporting information to provide some of the explanation. It has also become clear that we’re interested in CO2 per flight. See the exercises for a graph on that more directly. 4.7 What’s gone wrong? It’s inevitable that you will type = in tests where you mean ==. Some functions have friendly messages, since this is so common. Others less so. Watch that case! We are using the function filter(), not the function Filter() which is something else entirely. if_else is a fussy version of the base function ifelse, that we use here to maximise use of tidyverse functions. If it warns you that the ‘false’ must be something, then it has your long-term interests at heart. It just means that it’s a different type to the ‘true’. Compare ifelse(1&lt;2,\"true\",NA) and if_else(1&lt;2,\"true\",NA) where NA is the code for missing. 4.8 Exercises 4.8.1 Questions Where can you look to see that top_states has 8 values? How does geom_text() know where to place the text on the graph? Adapt geom_text() to shift all labels up 2 (million tonnes!). (Hint: Check the help file.) Adapt the final graph to show the smallest 8 states instead. (Hints: What’s the most likely counterpart to slice_max? Closely-related functions are often to be found in the same help file, so checking the help for a function you know might help you find similar ones that you don’t.) Adapt the graph to show year on the x-axis and CO2/flight on the y-axis. (Hints: Mostly changing the first aes() and deleting some elements. For a pretty graph you might google how to hide the legend “ggplot hide legend”, and how to set the breaks on the x-axis.) 4.8.2 Answers The environment pane, but you need to scroll down to “values” because it’s a simple vector. geom_text() inherits all aesthetics from the opening ggplot(..., aes(...)), which can be supplemented or over-written by its own aes(). In fact you could put the label= into the first aes(). geom_text(aes(label = state_label), nudge_y = 2). If you put the nudge_y inside the aes() you get an error (‘Ignoring unknown aesthetics: nudge_y’) because it’s not something that can vary with the values of a variable (“not an aesthetic”). geom_text_repel uses call-out lines when it can’t get the text close. You could experiment with making these a less confusing colour. You might also drop the ‘millions’. small_states &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # in year 2019 slice_min(TF, n = 8) %&gt;% # smallest 8 pull(STATE_NAME) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% small_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + ggrepel::geom_text_repel(aes(label = state_label)) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the least-busy states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) ## Warning: ggrepel: 3 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps A pedant might say this shouldn’t be a line chart, but here’s one possibility. We’ll see a different version of this in section (TBD) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(YEAR, CO2_QTY_TONNES/TF, colour = STATE_NAME)) + geom_path() + # I decided the points looked too heavy ggrepel::geom_text_repel(aes(label = state_label)) + theme(legend.position = &quot;none&quot;) + # turn off legend scale_x_continuous(breaks = 2010:2019, minor_breaks = NULL) + # control the breaks labs(x = &quot;Year&quot;, y = bquote(~CO[2]~&quot; (tonnes per flight)&quot;), title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) If even that doesn’t work, and for some reason the rda file is not in your data folder, go back to section 3.1, load the Excel file again, and find the code to summarise over years.↩︎ that’s a vertical bar, not an ‘l’ or ‘I’↩︎ and we use lower case for the variable name, because that’s our preference, even if the imported names don’t do this↩︎ "],["sortbars.html", "Chapter 5 Sorting Bars, Saving Graphs, Facets 5.1 The simple sorted bar chart - more on CO2 5.2 Saving a plot 5.3 Plotting more than one year 5.4 What’s gone wrong? 5.5 Exercises 5.6 Extended Exercises", " Chapter 5 Sorting Bars, Saving Graphs, Facets While nothing beats a well hand-crafted chart, there are times when you want to just run the code and get a quick update, as a .png say. In this chapter we see how to do a classic sorted-bar chart and to save it to a file for use elsewhere. We need slightly different methods for simple bar charts and more complex ones. In this chapter, you’ll be introduced to: geom_col(), dodge, reorder(), ggsave(), as.factor(), factor(), arrange(), facet_wrap(), select, plots pane 5.1 The simple sorted bar chart - more on CO2 A classic visualisation is the bar chart, sorted from longest to shortest. With ggplot there are a couple of ways to get a bar chart. If you want ggplot to count the rows for you, use geom_bar. Here we already have values for the length of the bar, so we use geom_col instead (for ‘column’ chart). For the simplest bar charts, there is a quick way to get the order you want. In place of state_label for the x-axis, you give reorder(state_label, CO2_QTY_TONNES), the second being the variable to sort by. If you find the bars a bit top-heavy, put a - in front of CO2_QTY_... to reverse the order. The final novelty in this graph is coord_flip(). Forty-something State names is a lot of text to cram onto the horizontal axis. So we flip the axes. You’ll need to decide if this trick works where you want to use the graph. We’ll see other ways to separate the labels on the axes in (TBD). ggplot(annual_co2 %&gt;% filter(YEAR == 2019), aes(reorder(state_label, CO2_QTY_TONNES), CO2_QTY_TONNES/1e6)) + geom_col() + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions in 2019&quot;, caption = &quot;Source: EUROCONTROL.&quot;) + coord_flip() If you were to google ‘ggplot ordered bar chart’, you might find references to ‘factors’. That becomes necessary, in place of reorder, when the charts are more complicated. We’ll look at that in section 5.3.1. 5.2 Saving a plot You might be looking at the bar chart and thinking that it’s the wrong proportions for your need (portrait rather than landscape, or vice versa) or you might be thinking the axis labels are still a bit squashed together. The proportions on your screen will depend on a number of things including the space you have allowed for the ‘Plots’ window. Now, the plots window has an export button which you could use. It allows for re-sizing, but that means you have to do the same manual intervention each time. We prefer to use ggsave() to save the most-recent plot, and at the same time set the aspect ratio. Usually it’s worth doing this before working too much on the font sizes, since you don’t really know if there’s a problem until you’ve seen the png. Finally, we use the graphs folder we created for the project. Square seems about right for this graph (the width includes the axis text); and having one of the dimensions around 15cm also seems to produce png that are good enough for reports and slides without being too big a file. ggsave(&quot;graphs/FirstSortedBars.png&quot;, width = 15, height = 15, units = &quot;cm&quot;) 5.3 Plotting more than one year I can think of four ways to plot more than one year, and there are no doubt more than that: as staggered bars, though we probably will have to work hard to make enough space; as ‘facets’, creating one sub-plot per year; as a few graphs, merged and aligned using an dedicated package like cowplot; as multiple graphs using a loop Number (3) is particularly useful for combining graphs of different variables, but it’s a bit heavy to deal with here. We’ll deal with (4) in section (TBD) when we look at loops. The first two we will demonstrate in the next sections. 5.3.1 Staggered bars, and factors We took some shortcuts in section 5.1, which will need sorting out for the staggered bars. First we need to choose a couple of years, since there certainly isn’t room for more than two. That’s a filter that we’ve seen before. Secondly, we used state_label before because it was prettier, but this only exists for 2019, so we have to go back to using STATE_NAME. It’s probably time to turn this name into title case once and for all. The separation by year is done in the aesthetic aes() as you might expect. We want the bars to be different colours by year. In this case it’s the fill that we specify; colour would add an outline to the bars. To get the bars side by side we set the position = \"dodge\" parameter in geom_col(). The least obvious, final step is that ‘year’ needs to be a discrete variable, whereas currently it’s num which is a continuous number. Slightly oddly, as.integer() doesn’t work: it’s still treated as continuous by ggplot, presumably because there are potentially still quite a lot of integers. We could convert to a string with as.character, but we need to start using factors, so let’s do that here. Factors in R were originally a way to save space with character variables in a dataset. In annual_co2 for example, rather than store ‘ALBANIA’ 10 times, for each row, a factor would give ALBANIA a numeric code and store that. The character strings become the levels. [Try z &lt;- as.factor(annual_co2$STATE_NAME) and see what is said for z in the environment pane. rm(z) to tidy up, if you wish.] For this example, we’ll convert YEAR on the fly, with an as.factor in the aes() call. annual_co2 &lt;- annual_co2 %&gt;% mutate(STATE_NAME = str_to_title(STATE_NAME)) ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2019)), aes(reorder(STATE_NAME, CO2_QTY_TONNES), CO2_QTY_TONNES/1e6, fill = as.factor(YEAR))) + # make discrete geom_col(position = &quot;dodge&quot;) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions in 2019&quot;, caption = &quot;Source: EUROCONTROL.&quot;, fill = &quot;Year&quot;) + # nicer label for legend coord_flip() Look closely at the graph. What is the sort order? Neither the 2010 nor the 2019 bars are actually in order. We’ve asked reorder to do too much. It seems to have sorted by the total of the 2 years, which is a reasonable thing to do in the circumstances. But I think that’s hard for the user of the graph to interpret, and I’d like the ordering to be by 2019. We can do this ordering with factors. First define a vector that is in the order we want, using arrange() to sort it. The desc() reverses the order. Then define a factor version of the state names, and insist that it’s in this fixed order. factor() is like as.factor() which we used in the previous chunk of code, but allows these extra parameters. # get the state names in the specific order that we want state_order &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # in year 2019 arrange(desc(CO2_QTY_TONNES)) %&gt;% # descending order pull(STATE_NAME) # create an ordered factor with this annual_co2 &lt;- annual_co2 %&gt;% mutate(ordered_states = factor(STATE_NAME, levels = state_order, ordered = TRUE)) ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2019)), aes(ordered_states, CO2_QTY_TONNES/1e6, fill = as.factor(YEAR))) + # make discrete geom_col(position = &quot;dodge&quot;) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions&quot;, caption = &quot;Ordering by 2019 emissions. Source: EUROCONTROL.&quot;, fill = &quot;Year&quot;) + # nicer label for legend coord_flip() 5.3.2 Chart facets, more years in the bar chart If you want the reader to compare things, a good rule of thumb is to make sure these things are all in the same graph. We’ve achieved this for comparisons between years and between countries. However, this is a bit of a squeeze, vertically, while there’s lots of empty space. Plus, it doesn’t look like this method would easily cope with a third year, say. ggplot provides a simple way to split charts into ‘facets’, which can sometimes be a way to show variation across a dimension with just a few values (2 or 3 years, say), while aligning the axes in a sensible way. There’s a bit of a twist in the notation: you can’t just mention a variable name (as you can in aes()), you need either to say vars(YEAR) or use a ‘formula’ notation starting with a tilde ~, which involves less typing so that’s what I’ve done here. The _wrap would allow wrapping onto multiple rows, but I just want one row here. ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2015, 2019)), aes(ordered_states, CO2_QTY_TONNES/1e6)) + geom_col() + facet_wrap(~YEAR, nrow = 1) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions&quot;, caption = &quot;Source: EUROCONTROL.&quot;) + coord_flip() facet_wrap has given all the x- and y-axes the same matching scale, and not bothered to repeat the y-axis labels. So it’s compact. The graph is not bad for comparing relative sizes of the larger States in a given year, and for seeing how the ranking changes. But it’s not that easy to compare amounts between years. However, we can use facets to split in a different way, if we arbitrarily put the States into two groups. Remember that [ ] is a way to select elements of the vector, in this case the first 19. We could equally have used head(state_order, 19), but the [1:19] is a model that is used more often. We turn off the scale-matching (scales = \"free\"), so really it’s two separate graphs, but with one piece of code. annual_co2 &lt;- annual_co2 %&gt;% mutate(size = if_else(ordered_states %in% state_order[1:19], &quot;Larger Emitters&quot;, &quot;Smaller Emitters&quot;)) ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2015, 2019)), aes(ordered_states, CO2_QTY_TONNES/1e6, fill = as.factor(YEAR))) + geom_col(position = &quot;dodge&quot;) + facet_wrap(~size, nrow = 1, scales = &quot;free&quot;) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions&quot;, caption = &quot;Source: EUROCONTROL.&quot;, fill = &quot;Year&quot;) + # nicer label for legend coord_flip() This certainly allows better comparisons of some of the mid-range emitters, and between years. Assuming you’re not after comparison of of Luxembourg and Finland, perhaps this is fit for purpose. There are many different distributions in flight data that have this long tail challenge: with most of the flights in a few airports, or a few countries, or by a few aircraft types. It’s easy enough to switch to a logarithmic scale, but that’s then often hard to read. Facets like this are a reasonable alternative, and adaptable. The results are a little distorted by the length of some of the names, for which a crude solution is in question (2) below. 5.4 What’s gone wrong? If your saved .png file seems very large, check that you specified the units. ggsave defaults to inches. If your axis labels are switched, remember that coord_flip will affect these. 5.5 Exercises 5.5.1 Questions Plot the bar chart of section 5.1 with the longest bars at the bottom. Plot the bar chart of section 5.1 without the 3 near-zero entries. (Hints: View the data. Filter on 2019 and choose a threshold.) Test the statement in the text that aes(..., colour=as.factor(YEAR)) gives an outline to the bars. Use select and ! to remove the size variable again. (Hint: Until you’re sure it works, don’t overwrite your dataset but make a temporary one, eg. start with z &lt;-.) Read the description of geom_bar in the help file. In the first bar chart, switch to using geom_bar instead - to get the same results. (Hint: A minor addition to the aes().) Save the final faceted bar chart to a png file. 5.5.2 Answers Use reorder(state_label, -CO2_QTY_TONNES). Use filter(YEAR == 2019 &amp; CO2_QTY_TONNES &gt; 100000). Did it? z &lt;- annual_co2 %&gt;% select(!size), then if it works, replace z. Replace geom_col with geom_bar and add weight = in front of CO2_QTY_TONNES/1e6, thus switching it from the y parameter to the weight. ggsave(\"graphs/2facet co2.png\", width = 15, height = 10, units = \"cm\") or some other appropriate proportions. Did you save the correct graph? Remember that by default it saves the last one plotted. 5.6 Extended Exercises In chapters 3 to 5 we’ve covered some basics of the R language, seen through the process of building graphs of some emissions data. Before moving on to new concepts, you might like to try some extended exercises. These do not introduce any new functions, but they might use new parameters for functions that you’ve already seen. So a good place to start if you’re stuck is the help file for the functions that you know about. If that doesn’t work, then there are some hints in section 5.6.2, deliberately placed slightly separate from the questions! This book also has a search function, for finding your way back to relevant sections - see the magnifying glass top left. Up and down arrows move from one find to the next. It’s good practice to periodically ‘clean’ your environment, that is, to remove all the data saved in it. This is essential before testing, because you often find that you’re relying on something that has been changed as you tweak and improve the code. Click the broom icon in the environment pane before you start the exercises. 5.6.1 Questions Starting from the final graph in chapter 4.5, move the label to 2015 and set the colour of the label to black. Building on (1) plot the 6 states with fewest flights in 2010, but which had more than 10,000 flights . There’s a bit of overlap, so use a different shape for each one. (Harder) Plot the monthly emissions for France from 2015 onwards as a bar chart. We haven’t done dates yet, so use a character string. 5.6.2 Hints If you’re starting from a ‘clean’ environment, you will need to load the data and re-create top_states. Search in this book for load( and top_states. Mostly a question of filtering. Don’t forget to check the text of your graph when it’s done. The help for geom_point lists which aesthetics are understood, so will help you find the parameter name for changing shape (in this case, it’s the obvious answer). You need to go back to the code that loads the original excel file. You’ll need to mutate to create a new variable combining two strings str_c (or paste), but you need to put this inside an if else to treat 10-12 different to the other months, or the text won’t sort correctly. Remember what we did to the bar charts when there was long text on the x-axis? In the end, it’s an ugly chart but let’s not worry too much about that. 5.6.3 Answers This exercise is really a lesson in assembling the code into one sequence - it gets a bit broken up in the book. You should probably include your library(tidyverse) statement at the top, for really complete code. Notice that we have two colour aesthetics: the default one which is by YEAR and one specifically for the geom_text_repel. The other catch is that, because the colour of the text is constant, it appears outside the aes(), not inside. # load the dataset load(&quot;data/annual_co2.rda&quot;) top_states &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # top in year 2019 slice_max(TF, n = 8) %&gt;% # top 8 pull(STATE_NAME) annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2015, str_to_title(STATE_NAME), &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + ggrepel::geom_text_repel(aes(label = state_label), colour = &quot;black&quot;) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) This is an exercise in logic in the filter. Did you remember to update the title, footnotes and the legend title? # load the dataset load(&quot;data/annual_co2.rda&quot;) selected_states &lt;- annual_co2 %&gt;% filter(YEAR == 2010 &amp; TF &gt; 10000) %&gt;% # more than 10k flights slice_min(TF, n = 6) %&gt;% # bottom 6 pull(STATE_NAME) annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2015, str_to_title(STATE_NAME), &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% selected_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point(aes(shape = STATE_NAME)) + geom_path() + ggrepel::geom_text_repel(aes(label = state_label), colour = &quot;black&quot;) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, shape = &quot;State&quot;, title = &quot;Emissions for the least-busy 6 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Least-busy&#39; means fewest flights in 2010, but more than 10k flights.&quot;) ## Warning: ggrepel: 2 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps You could load from Excel and immediately filter. I do it in two steps, because then I can check the correct field names for the filter and mutate using the environment pane. This is an exercise in building up quite complex statements from simple functions. Formatting with line breaks should help a lot. # go back to the excel file aviation_co2 &lt;- readxl::read_excel(&quot;data/CO2_emissions.xlsx&quot;, sheet = &quot;DATA&quot;) # just the data that we need monthly_co2 &lt;- aviation_co2 %&gt;% filter(YEAR &gt;= 2017 &amp; STATE_NAME == &quot;FRANCE&quot;) %&gt;% mutate(date = if_else(MONTH &lt; 10, str_c(YEAR, &quot; &quot;, MONTH), str_c(YEAR, &quot; &quot;, MONTH))) #a pseudo-date # then the basic ggplot(data, aes(x,y)) + geometry... ggplot(monthly_co2, aes(date, CO2_QTY_TONNES/1e6)) + geom_col() + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions of France&quot;, caption = &quot;Source: EUROCONTROL.&quot;) + coord_flip() "],["loopsfunctions.html", "Chapter 6 Loops, functions and SID data 6.1 Vectors and Lists 6.2 Functions and Loops 6.3 Country data", " Chapter 6 Loops, functions and SID data In the next few chapters we move away from tweaking code for graphs and get more hands-on with manipulating data. With another software language, this might be the time to discuss loops. In a manner of speaking we will be looping, but in a very R way. R does have a ‘for..next’ syntax, but it’s a little like the chips on the menu of an Indian restaurant: most of the time you’re better off with something else. We’ll cover loops in 4 ways: 2 explicit loops, the classic R lapply and the tidyverse map-reduce pairing; and 2 implicit loops, because half the time in R you hardly notice you’re looping, vectors and group_by for looping within rows of a dataframe. group_by and its associated functions are a major toolset, so we will cover them in the next chapter, the other three in this one. Loops go hand-in-hand with functions, so this chapter also takes some first steps in defining our own. There’s a wealth of flight data to download in the STATFOR Interactive Dashboard or ‘SID’ to its friends. The Excel downloads look fiddly to process into something you might handle in R, but we’ll see how to do this quite quickly with a few short loops. In this chapter, you’ll be introduced to: lapply, rep, function, pmin, pmax, is.list, unlist, read_xlsx, map, pmap, crossing By the end of this chapter you will know how to write functions to extract structured, but complex data from a directory full of spreadsheets. 6.1 Vectors and Lists We met vectors in passing in Chapter 2: 1:50. We saw other ways to construct them in section 3.3: c(1, 5, 10). But we can do more than just use them for filtering. Many functions in R are ‘vector friendly’, in the sense that they will operate on an entire vector, and return a vector result. This can then be used in another function. The use of vectors is a sort of looping through a set of values. Or if you prefer the Excel analogy, it’s like filling a column of cells with a sequence of values and then creating another with a set of formulas using those vales, but without all the repetitious clicking, dragging and filling. c(1, 3, 7) + 2 ## [1] 3 5 9 1:4 + 1:2 # values of the second vector are recycled ## [1] 2 4 4 6 1:3^2 # ^ happens before : ## [1] 1 2 3 4 5 6 7 8 9 (1:3)^2 ## [1] 1 4 9 sin(pi/1:4) %&gt;% round(3) # = round(sin(pi/1:4), 3) ## [1] 0.000 1.000 0.866 0.707 round(pi, 0:4) # vectors can work in surprising places ## [1] 3.0000 3.1000 3.1400 3.1420 3.1416 str_c(&quot;Year_&quot;, unique(aviation_co2$YEAR)) ## [1] &quot;Year_2010&quot; &quot;Year_2011&quot; &quot;Year_2012&quot; &quot;Year_2013&quot; &quot;Year_2014&quot; &quot;Year_2015&quot; ## [7] &quot;Year_2016&quot; &quot;Year_2017&quot; &quot;Year_2018&quot; &quot;Year_2019&quot; &quot;Year_2020&quot; The examples illustrate that there are still some rules of precedence: : is evaluated after ^, but before +. They also show ‘recycling’: 1:4 needs a length 4 vector to be added to it, so the values in 1:2 are recycled. Strictly speaking, this is happening with the first example (the 2 is being recycled into a vector of length 3), and also in the last example with “Year_”. If the recycling can’t be done neatly, you get an error or a warning. [Evaluate 1:4 + 1:3.] There are plenty of other ways to create simple vectors when you need them. seq(10, 50, by = 5) # when 10:50 won&#39;t do ## [1] 10 15 20 25 30 35 40 45 50 rep(4, 3) # repeat 3 times ## [1] 4 4 4 seq.Date(as.Date(&quot;2020/1/15&quot;), as.Date(&quot;2020/12/15&quot;), by = &quot;month&quot;) # ides of 2020 ## [1] &quot;2020-01-15&quot; &quot;2020-02-15&quot; &quot;2020-03-15&quot; &quot;2020-04-15&quot; &quot;2020-05-15&quot; ## [6] &quot;2020-06-15&quot; &quot;2020-07-15&quot; &quot;2020-08-15&quot; &quot;2020-09-15&quot; &quot;2020-10-15&quot; ## [11] &quot;2020-11-15&quot; &quot;2020-12-15&quot; letters # &#39;letters&#39; and &#39;LETTERS&#39; are built in to base R ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; # and by combining others a &lt;- c(1, 1, 2) b &lt;- c(3, 5, 8, 13) c(a, b) ## [1] 1 1 2 3 5 8 13 While vectors and vector-savvy functions are, in a sense, loops with an R flavour, it’s probably better to think of them as working ‘in parallel’ calculated all at the same time, rather than one by one as they would be in a ‘for…next’ loop. (TBD) Every element in a vector must be the same type (character, numeric, and here we introduce integer with an L after it, standing for ‘long’). If you need something more general than that, then list is for you. A ‘list’ can contain almost anything, including vectors and other lists. a &lt;- c(787, &quot;neo&quot;, 320) str(a) # every element was silently converted to character ## chr [1:3] &quot;787&quot; &quot;neo&quot; &quot;320&quot; b &lt;- list(72, &quot;CO2&quot;, 380L, c(&quot;Embraer&quot;, &quot;ATR&quot;)) str(b) # list is less fussy ## List of 4 ## $ : num 72 ## $ : chr &quot;CO2&quot; ## $ : int 380 ## $ : chr [1:2] &quot;Embraer&quot; &quot;ATR&quot; We already saw how to extract specific elements of a vector, in section 3.3. [How would you extract the first and third elements of vector a? Does the same work for the list b?] R has some more tricks up its sleeve to help with referring to elements. They can all have names. Or some can have names, and some not, as in the following examples. To extract elements by their names you can use the square brackets: so within the brackets you can give names or numbers. Lists have an extra trick, the $. # putting brackets around the code prints the result as well as executing it (a &lt;- c(a = 7, 3, b = 7, c = 800)) ## a b c ## 7 3 7 800 (b &lt;- list(p = 0.05, pi = 3.14, pie = &quot;apple&quot;, pier = &quot;Bournemouth&quot;, piers = c(&quot;Weston&quot;, &quot;Wigan&quot;))) ## $p ## [1] 0.05 ## ## $pi ## [1] 3.14 ## ## $pie ## [1] &quot;apple&quot; ## ## $pier ## [1] &quot;Bournemouth&quot; ## ## $piers ## [1] &quot;Weston&quot; &quot;Wigan&quot; names(b) ## [1] &quot;p&quot; &quot;pi&quot; &quot;pie&quot; &quot;pier&quot; &quot;piers&quot; a[c(&quot;a&quot;, &quot;b&quot;)] ## a b ## 7 7 b[&quot;pie&quot;] ## $pie ## [1] &quot;apple&quot; b$pier ## [1] &quot;Bournemouth&quot; b$pier[2] ## [1] NA Does the $ method work for vectors? [Try a$c] It’s no coincidence that we use $ to extract an element from a list, as well as (in section 3.2) extracting a field from a dataset, but I’ll leave you to wonder why that is. 6.1.1 Exercises 6.1.1.1 Questions What code generates the vector 2 6 10 14? What code generates the vector 1 2 1 2 1 2 1 2? What code generates the vector \"A1\" \"B2\" \"C3\" \"D4\" \"E5\"? What code generates the vector 1.000 0.500 0.333 0.250 0.200? What code generates the vector \"1/1\" \"1/2\" \"1/3\" \"1/4\" \"1/5\"? Create a named vector containing the numbers 1 to 26 named with upper case letters of the alphabet (and not by typing them out!). 6.1.1.2 Answers For the first 5 there are several answers that work. The ones here are relatively compact. seq(2, 14, by = 4), or c(2, 6, 10, 14) but feels a bit manual, or 4 * 1:4 - 2 if you’re into cryptic code (hiding both the start and the end in a bit of maths) rep(1:2, 4) str_c(LETTERS[1:5], 1:5) 1/1:5 str_c(\"1/\", 1:5) The names function can be used to set as well as to get the names. z &lt;- 1:26 names(z) &lt;- LETTERS 6.2 Functions and Loops We have already seen plenty of example of using functions, passing parameters and returning their values. How do we define our own functions? We said that almost everything you do with R is use functions. Well, you even define a function using function(), which is a special sort of function: my_function &lt;- function(parameters) { code to apply to the parameters }. The parameters are all of the form name or name=expression if you want to give a default value. The value returned is the result of the last phrase of code, or what is explicitly in a return() statement. 6.2.1 First function: bi-directional airport pairs For our first function, think about converting uni-directional flight data to bi-directional: you want to merge Heathrow&gt;JFK with JFK&gt;Heathrow. Often we do this by just taking the alphabetically-first one first. Our first function is to help that. In this first example we hit the ground running, introducing an optional but powerful parameter. You can use an ellipsis, ..., in the parameter list of your function; then echo it in a function within your function. Then a user can pass, in the function call, parameters directly to the inner function call. We use it here to allow us to pass the sep= parameter (or others) to the str_c function. alpha_str_c &lt;- function(n1, n2, ...) { # concatenate in parallel each element of these two vectors # but with the smaller of the two elements first. str_c(pmin(n1, n2), pmax(n1, n2), ...) } #some airports ap_list1 &lt;- c(&quot;EGLL&quot;, &quot;KJFK&quot;, &quot;LFPG&quot;) ap_list2 &lt;- c(&quot;KJFK&quot;, &quot;EGLL&quot;, &quot;LEMD&quot;) alpha_str_c(ap_list1, ap_list2, sep = &quot;&lt;&gt;&quot;) ## [1] &quot;EGLL&lt;&gt;KJFK&quot; &quot;EGLL&lt;&gt;KJFK&quot; &quot;LEMD&lt;&gt;LFPG&quot; # it recycles values if the vectors are different lengths # this is inherited directly from the two functions that it calls alpha_str_c(&quot;EHAM&quot;, ap_list2, sep = &quot;&lt;&gt;&quot;) ## [1] &quot;EHAM&lt;&gt;KJFK&quot; &quot;EGLL&lt;&gt;EHAM&quot; &quot;EHAM&lt;&gt;LEMD&quot; This is a pretty simple function. It doesn’t even define any of its own data, just calls another function. We could have assigned the result to a variable pairs &lt;- or something like that. The result would be no different, and the variable is forgotten once you leave the function anyway. [Try this.] Using ... in this way saves you the bother of deciding how and which parameters to pass on to the str_c, and can open your function to being used in ways you had not first intended. Even a short function like this is worth writing. Once you’ve checked it, you can use it several times and be confident there are no typos (it would be easy to type pmin twice, for example). Note the way the lines are broken in the function definition. R really doesn’t mind, but they’re easier to read if they’re all formatted in the same way. The tidyverse style guide says that function names should be verbs. In the spirit of this, our ‘alphabetize and concatenate’ function name becomes alpha_str_c, shortening “concatenate” for obvious reasons. We could have dropped the str_ bit, but it’s a reminder of which parameters can be passed with .... The style guide means that, if you’re trying to remember a tidyverse function, in theory the mental search space can be cut down to verbs. In practice, English doesn’t help much since so many words are both nouns and verbs, as crossword-setters exploit with glee. What if we sometimes want to combine in reverse alphabetical order? Rather than have a separate function, we extend alpha_str_c using a logical TRUE or FALSE parameter, and an if-else statement. The relevant help section here is if for the language construct, rather than for the function dplyr::if_else (which you saw in section 4.5). alpha_str_c &lt;- function(n1, n2, asc = TRUE, ...) { # concatenate in parallel each element of these two vectors # but with the smaller of the two elements first. if (asc) { str_c(pmin(n1, n2), pmax(n1, n2), ...) } else { str_c(pmax(n1, n2), pmin(n1, n2), ...) } } #some airports ap_list1 &lt;- c(&quot;EGLL&quot;, &quot;KJFK&quot;, &quot;LFPG&quot;) alpha_str_c(&quot;EFHK&quot;, ap_list1, sep = &quot;&lt;&gt;&quot;) ## [1] &quot;EFHK&lt;&gt;EGLL&quot; &quot;EFHK&lt;&gt;KJFK&quot; &quot;EFHK&lt;&gt;LFPG&quot; alpha_str_c(&quot;EFHK&quot;, ap_list1, sep = &quot;&lt;&gt;&quot;, asc = FALSE) ## [1] &quot;EGLL&lt;&gt;EFHK&quot; &quot;KJFK&lt;&gt;EFHK&quot; &quot;LFPG&lt;&gt;EFHK&quot; That’s quite a lot of curly brackets for one if-else. In fact, when all you have within them is a single statement, you can omit the “{}”. [Exercise: Remove the excess brackets and show the result is unchanged.] The example also illustrates that, once you’re passing a parameter “by name”, the order doesn’t matter: asc appears before ... and hence before sep in the parameter list, but we pass them in any order we like. When using a function, passing parameters by position is more compact. Your variable names should make clear what is in them, anyway. So it should be clear what you’re passing to the function, here two airport lists. What the function calls them internally is irrelevant. However, when passing a logical parameter as in the last example, it isn’t at all clear what the parameter means from your call to the function, without opening the function. alpha_str_c(\"EFHK\", ap_list1, FALSE, sep = \"&lt;&gt;\") uses the implicit position of the asc parameter and gives the same result. [Try this.] But when you come back to the code in a week or two, will you remember what the FALSE means? Better then, to be explicit with the parameter name, as we were in the code chunk above. 6.2.2 Looping with functions Now that we can define functions, we can try out a basic R loop. There are many variants to the ‘apply a function over a list of values’ function lapply [read the help file]. Each gives different sorts of result, and they can be difficult to remember. As a result, I use lapply probably more often than I should, and just deal with the fact that it returns a list each time. One variety of function doesn’t get a name (function_name &lt;-) at all. These anonymous, temporary functions are defined just to be passed as parameters to another function, and are forgotten afterwards. Here, we pass anonymous functions to lapply, which takes a list and applies a function to the list one at a time, returning a list of results. listed_loop &lt;- lapply(1:10, function(x) {x^2 + x}) is.list(listed_loop) ## [1] TRUE unlisted_loop &lt;- lapply(1:10, function(x) {x^2 + x}) %&gt;% unlist() # which could also have been listed_loop %&gt;% unlist() is.list(unlisted_loop) ## [1] FALSE unlisted_loop ## [1] 2 6 12 20 30 42 56 72 90 110 Check in the Environment pane, or by printing to the console, how these are different. [Why is unlisted_loop in a different part of the Environment pane?] Which of the two you need will depend on what you’re developing, but I think the second is more frequent. For a second example, we wrap this lapply-unlist pairing up inside a function. Before you run this code chunk, or read on to the end, describe in words what it will do, given two vectors of text. The answer is a couple of paragraphs earlier. combos_str_c &lt;- function(s1, s2, ...){ lapply(s1, function(x) str_c(x, s2, ...)) %&gt;% unlist() } combos_str_c(c(&quot;A319&quot;, &quot;A320&quot;, &quot;A321&quot;), c(&quot;ceo&quot;, &quot;neo&quot;), sep=&quot;&quot;) ## [1] &quot;A319ceo&quot; &quot;A319neo&quot; &quot;A320ceo&quot; &quot;A320neo&quot; &quot;A321ceo&quot; &quot;A321neo&quot; ‘Combos’ is short for ‘combinations’. Rather than just pasting elements of the vectors in pairs, as in alpha_str_c, it creates all combinations of the two. In the example this creates a list of ‘current engine’ and ‘new engine’ options of Airbus aircraft. You’ll have to check elsewhere if all 6 exist! 6.2.3 Exercises 6.2.3.1 Questions What would happen if you used min and max instead, in alpha_str_c? Read the help file first to decide. Then check this by adapting the code. What happens if you pass lists with respectively 3 and 4 airport codes in them? Why does it happen twice? What other parameters can you pass to alpha_str_c? Inside alpha_str_c assign the result to a variable. Does anything change? How would you adapt alpha_str_c to use &lt;&gt; as a default separator? With this new version, do the two examples of its use, in the code chunks above, still work? How can you make the examples even more compact? Building on combos_str_c write a function to create all bidirectional combinations of two lists of airport names or codes, eg c(\"EGLL\", \"LEMD\", \"LFPG\", \"LTFM\") and c(\"LIRF\", \"EHAM\", \"LFPO\", \"EDDF\")? Extend the function from (7) to find all possible routes between the airports in a single list. (Hint: It means adding just 5 characters to the code, including spaces.) Add a parameter to allow this function to return, either the full list or with duplicates removed. (Hint: You’ll need a function you saw in section 3.2) 6.2.3.2 Answers min combines whatever it is passed, and takes the minimum of them all. So you get the first airport of all, and the last. I have used min instead of pmin more times than I’d like to say. Think “parallel min need pmin”. R will ‘recycle’ elements to match the lengths of the vectors, but in this case, warn you that that is happening. It’s the pmin and pmax that are doing the recycling, not the str_c, so the error is found twice. From the help file, str_c and therefore alpha_str_c also accepts collapse. [Experiment with using the collapse parameter.] alpha_str_c &lt;- function(n1, n2, ...) {pairs &lt;- str_c(pmin(n1, n2), pmax(n1, n2), ...)} and no, no difference. The function returns the last thing calculated. alpha_str_c &lt;- function(n1, n2, sep=\"&lt;&gt;\", ...) {str_c(pmin(n1, n2), pmax(n1, n2), sep = sep, ...)} You need to define the default in your own function, and now pass it explicitly to the new one. I don’t really like sep = sep which feels an odd thing to write, but it would be stranger still to have a separator parameter than was called something else. Yes they do, but now alpha_str_c(\"EHAM\", ap_list2) is enough. This merges the two functions we saw earlier. The asc and sep parameters both work through the ellipsis ... even though they’re from different functions: this demonstrates the power of ‘nested’ ellipsis. alpha_combos_str_c &lt;- function(s1, s2, ...){ lapply(s1, function(x) alpha_str_c(x, s2, ...)) %&gt;% unlist() } ap_list1 &lt;- c(&quot;EGLL&quot;, &quot;LEMD&quot;, &quot;LFPG&quot;, &quot;LTFM&quot;) ap_list2 &lt;- c(&quot;LIRF&quot;, &quot;EHAM&quot;, &quot;LFPO&quot;, &quot;EDDF&quot;) alpha_combos_str_c(ap_list1, ap_list2, sep = &quot;&lt;&gt;&quot;) ## [1] &quot;EGLL&lt;&gt;LIRF&quot; &quot;EGLL&lt;&gt;EHAM&quot; &quot;EGLL&lt;&gt;LFPO&quot; &quot;EDDF&lt;&gt;EGLL&quot; &quot;LEMD&lt;&gt;LIRF&quot; ## [6] &quot;EHAM&lt;&gt;LEMD&quot; &quot;LEMD&lt;&gt;LFPO&quot; &quot;EDDF&lt;&gt;LEMD&quot; &quot;LFPG&lt;&gt;LIRF&quot; &quot;EHAM&lt;&gt;LFPG&quot; ## [11] &quot;LFPG&lt;&gt;LFPO&quot; &quot;EDDF&lt;&gt;LFPG&quot; &quot;LIRF&lt;&gt;LTFM&quot; &quot;EHAM&lt;&gt;LTFM&quot; &quot;LFPO&lt;&gt;LTFM&quot; ## [16] &quot;EDDF&lt;&gt;LTFM&quot; alpha_combos_str_c(ap_list1, ap_list2, sep = &quot;&lt;&gt;&quot;, asc = FALSE) ## [1] &quot;LIRF&lt;&gt;EGLL&quot; &quot;EHAM&lt;&gt;EGLL&quot; &quot;LFPO&lt;&gt;EGLL&quot; &quot;EGLL&lt;&gt;EDDF&quot; &quot;LIRF&lt;&gt;LEMD&quot; ## [6] &quot;LEMD&lt;&gt;EHAM&quot; &quot;LFPO&lt;&gt;LEMD&quot; &quot;LEMD&lt;&gt;EDDF&quot; &quot;LIRF&lt;&gt;LFPG&quot; &quot;LFPG&lt;&gt;EHAM&quot; ## [11] &quot;LFPO&lt;&gt;LFPG&quot; &quot;LFPG&lt;&gt;EDDF&quot; &quot;LTFM&lt;&gt;LIRF&quot; &quot;LTFM&lt;&gt;EHAM&quot; &quot;LTFM&lt;&gt;LFPO&quot; ## [16] &quot;LTFM&lt;&gt;EDDF&quot; We just want the second list to be the same as the first one, by default, so like this. alpha_combos_str_c &lt;- function(s1, s2 = s1, ...){ lapply(s1, function(x) alpha_str_c(x, s2, ...)) %&gt;% unlist() } ap_list1 &lt;- c(&quot;EGLL&quot;, &quot;LEMD&quot;, &quot;LFPG&quot;, &quot;LTFM&quot;) alpha_combos_str_c(ap_list1, sep = &quot;&lt;&gt;&quot;) ## [1] &quot;EGLL&lt;&gt;EGLL&quot; &quot;EGLL&lt;&gt;LEMD&quot; &quot;EGLL&lt;&gt;LFPG&quot; &quot;EGLL&lt;&gt;LTFM&quot; &quot;EGLL&lt;&gt;LEMD&quot; ## [6] &quot;LEMD&lt;&gt;LEMD&quot; &quot;LEMD&lt;&gt;LFPG&quot; &quot;LEMD&lt;&gt;LTFM&quot; &quot;EGLL&lt;&gt;LFPG&quot; &quot;LEMD&lt;&gt;LFPG&quot; ## [11] &quot;LFPG&lt;&gt;LFPG&quot; &quot;LFPG&lt;&gt;LTFM&quot; &quot;EGLL&lt;&gt;LTFM&quot; &quot;LEMD&lt;&gt;LTFM&quot; &quot;LFPG&lt;&gt;LTFM&quot; ## [16] &quot;LTFM&lt;&gt;LTFM&quot; alpha_combos_str_c(ap_list1, sep = &quot;&lt;&gt;&quot;, asc = FALSE) ## [1] &quot;EGLL&lt;&gt;EGLL&quot; &quot;LEMD&lt;&gt;EGLL&quot; &quot;LFPG&lt;&gt;EGLL&quot; &quot;LTFM&lt;&gt;EGLL&quot; &quot;LEMD&lt;&gt;EGLL&quot; ## [6] &quot;LEMD&lt;&gt;LEMD&quot; &quot;LFPG&lt;&gt;LEMD&quot; &quot;LTFM&lt;&gt;LEMD&quot; &quot;LFPG&lt;&gt;EGLL&quot; &quot;LFPG&lt;&gt;LEMD&quot; ## [11] &quot;LFPG&lt;&gt;LFPG&quot; &quot;LTFM&lt;&gt;LFPG&quot; &quot;LTFM&lt;&gt;EGLL&quot; &quot;LTFM&lt;&gt;LEMD&quot; &quot;LTFM&lt;&gt;LFPG&quot; ## [16] &quot;LTFM&lt;&gt;LTFM&quot; This calls for a logical parameter, and finally a good use for defining a variable within the function. The final else pairs is really saying else return(pairs). If you omit the else pairs completely then, when uniq=FALSE the if evaluates to nothing, so the function returns nothing. [Try this.] alpha_combos_str_c &lt;- function(s1, s2=s1, uniq = FALSE, ...){ pairs &lt;- lapply(s1, function(x) alpha_str_c(x, s2, ...)) %&gt;% unlist() if (uniq) unique(pairs) else pairs } ap_list1 &lt;- c(&quot;EGLL&quot;, &quot;LEMD&quot;, &quot;LFPG&quot;, &quot;LTFM&quot;) alpha_combos_str_c(ap_list1, sep = &quot;&lt;&gt;&quot;) ## [1] &quot;EGLL&lt;&gt;EGLL&quot; &quot;EGLL&lt;&gt;LEMD&quot; &quot;EGLL&lt;&gt;LFPG&quot; &quot;EGLL&lt;&gt;LTFM&quot; &quot;EGLL&lt;&gt;LEMD&quot; ## [6] &quot;LEMD&lt;&gt;LEMD&quot; &quot;LEMD&lt;&gt;LFPG&quot; &quot;LEMD&lt;&gt;LTFM&quot; &quot;EGLL&lt;&gt;LFPG&quot; &quot;LEMD&lt;&gt;LFPG&quot; ## [11] &quot;LFPG&lt;&gt;LFPG&quot; &quot;LFPG&lt;&gt;LTFM&quot; &quot;EGLL&lt;&gt;LTFM&quot; &quot;LEMD&lt;&gt;LTFM&quot; &quot;LFPG&lt;&gt;LTFM&quot; ## [16] &quot;LTFM&lt;&gt;LTFM&quot; alpha_combos_str_c(ap_list1, sep = &quot;&lt;&gt;&quot;, uniq = TRUE) ## [1] &quot;EGLL&lt;&gt;EGLL&quot; &quot;EGLL&lt;&gt;LEMD&quot; &quot;EGLL&lt;&gt;LFPG&quot; &quot;EGLL&lt;&gt;LTFM&quot; &quot;LEMD&lt;&gt;LEMD&quot; ## [6] &quot;LEMD&lt;&gt;LFPG&quot; &quot;LEMD&lt;&gt;LTFM&quot; &quot;LFPG&lt;&gt;LFPG&quot; &quot;LFPG&lt;&gt;LTFM&quot; &quot;LTFM&lt;&gt;LTFM&quot; 6.2.3.3 Extended question Some theory you didn’t want to know. A default value provided in a function definition is often called a ‘promise’. It is only evaluated at the moment that it is needed, not at, say, some hypothetical moment of compilation. Knowing that, predict the result of calling promises(), without specifying any parameters. Then try it in the console. a &lt;- 5 promises &lt;- function(a = 1, b = a^2) { a &lt;- 2 b #short for return(b) } 6.3 Country data The first data that we look at from SID is the ‘monthlies’. This tab in the dashboard shows monthly total flights in a choice of airspaces, for any period from Jan 2005. You can ask for one or more market segments, too, so low-cost, or business aviation etc. And the units can be switched to flight minutes rather than flights. Data can be exported to Excel format. Monthly data in the SID 6.3.1 Load one country’s monthly data An ‘airspace’ here is usually a country’s airspace: think of extending the border on the ground upwards. In some cases, such as Belgium and Luxembourg, they share an airspace. In others an airspace is split: Spain has (continental) Spain and Canary Islands. There are also aggregates, such as ECAC, for which you’ll find details on line, for example region definitions. Being airspaces, they have overflights: flights which neither depart, nor land in the country but just pass through the airspace. There are two little catches to the SID data. Firstly, the Excel export comes in a number of tables one after the other on a single worksheet. Secondly, the field names of each table are horrible, and use merged cells. We’ve extracted some data for the all-cargo market segment. As a first example, we extract a single table from within the Excel file. [Download some yourself and inspect the Excel file.] For this book, as with the emissions data, we provide a snapshot on-line which you can download like this. You only need to do this once. url &lt;- &quot;https://github.com/david6marsh/flights_in_R/raw/main/data/SID_Monthly_AllCargo.xlsx&quot; download.file(url, &quot;data/SID_Monthly_AllCargo.xlsx&quot;, mode = &quot;wb&quot;) The column names are month, and three values for each of arrival, departure, (one airport in the airspace and one not), internal (both airports in), overflight (neither) and total. We use combos_str_c to generate some more useful column names. Loading one table is then just a matter of saying how many rows to ignore at the top (skip=) and how many rows to load (n_max). # this adds the output of combos_str_c to a vector that already has &#39;month&#39; in it # the indentation should help highlight that monthly_cols &lt;- c(&quot;month&quot;, combos_str_c(c(&quot;A&quot;, &quot;D&quot;, &quot;I&quot;, &quot;O&quot;, &quot;T&quot;), c(&quot;_last&quot;, &quot;&quot;, &quot;_growth&quot;), sep=&quot;&quot;)) monthlies &lt;- readxl::read_xlsx(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, skip = 9, n_max = 14, col_names = monthly_cols) The month column has been correctly interpreted as a date. We’ll look at dates and date-time variables in more detail in section TBD. For the moment, if you see POSIXct then this is a format for date-time variables. In this chapter, the dates look after themselves. 6.3.2 Load monthly data for multiple countries That didn’t take much code. If we want to load multiple countries, then we need to wrap up the extraction into a function. At the same time, we can pick up some metadata: market segment and units from the top of the file, and the airspace name from just before the data. These are obtained by selecting a specific range (here just one cell at a time). extract_monthlies &lt;- function(file, skip, n_rows){ monthly_cols &lt;- c(&quot;month&quot;, combos_str_c(c(&quot;A&quot;, &quot;D&quot;, &quot;I&quot;, &quot;O&quot;, &quot;T&quot;), c(&quot;_last&quot;, &quot;&quot;, &quot;_growth&quot;), sep=&quot;&quot;)) meta_segment &lt;- readxl::read_xlsx(file, range = &quot;A2&quot;, col_names = &quot;segment&quot;) meta_units &lt;- readxl::read_xlsx(file, range = &quot;A4&quot;, col_names = &quot;units&quot;) # need to create an Excel reference &quot;An&quot; where n is just before skip meta_airspace &lt;- readxl::read_xlsx(file, range = str_c(&quot;A&quot;, skip - 2), col_names = &quot;airspace&quot;) monthlies &lt;- readxl::read_xlsx(file, skip = skip, n_max = n_rows, col_names = monthly_cols) # this is a quick way to return a dataframe reasonable column order # the first 3 get recycled as usual, to the same length as monthlies bind_cols(meta_airspace, meta_segment, meta_units, monthlies) } # check this works, as before but with added metadata monthlies &lt;- extract_monthlies(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, skip = 9, n_rows = 14) By inspecting the Excel file, we need skips c(9, 28, 47, ..., 199). [How do you generate that with seq?] For the looping we are spoiled for choice. We could use lapply, as we saw earlier. That is simple to remember. The anonymous function isn’t pretty, but that’s hardly grounds for dismissal. We can use purrr::map which is very similar, and has a nicer format for anonymous functions. It also reminds us that there’s a wide variety of map functions around. Something for most purposes. pmap is one of the purrr::map family. It has the nice property of enabling us to specify a set of locations for the data, then very compactly extract from those locations: implicitly taking a field file from the extract_locs dataframe and passing it to the file parameter of a function. And similarly for the other parameters. Remember it as “parameter or parallel map? pmap”. # V1. Use lapply all_monthlies_v1 &lt;- lapply(seq(9, 199, by = 19), function(x) extract_monthlies(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, x, 14)) %&gt;% bind_rows() # V2. With &#39;map&#39; the anonymous function is more compact all_monthlies_v2 &lt;- map(seq(9, 199, by = 19), ~extract_monthlies(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, ., 14)) %&gt;% bind_rows() identical(all_monthlies_v1, all_monthlies_v2) ## [1] TRUE # V3. # Build a set of parameters for the extract extract_locs &lt;- data.frame(file = &quot;data/SID_Monthly_AllCargo.xlsx&quot;, skip = seq(9, 199, by = 19), n_rows = 14) # Then pmap passes them by name to your function all_monthlies_v3 &lt;- extract_locs %&gt;% pmap(extract_monthlies) %&gt;% bind_rows() identical(all_monthlies_v1, all_monthlies_v3) ## [1] TRUE Code that keeps data (or metadata, like filenames) in datasets rather than in parameter lists, is an advantage: it will be easier to extend to other files (as we will see TBD); and it means that you can also save it and hence keep a record. None of the solutions is ‘wrong’, but the third one is the one I prefer. 6.3.3 Load multiple files The third version is most interesting because we actually want to load data from multiple files. For this book, there’s an admin step first, which is to get them from the web onto your machine. We do this with a list and a function that does the looping for us. file_segments &lt;- c(&quot;LowCost&quot;, &quot;TradSched&quot;, &quot;BusAvn&quot;, &quot;AllCargo&quot;) urls &lt;- str_c(&quot;https://github.com/david6marsh/flights_in_R/raw/main/data/SID_Monthly_&quot;, file_segments, &quot;.xlsx&quot;) local_names &lt;- str_c(&quot;data/SID_Monthly_&quot;, file_segments, &quot;.xlsx&quot;) download.file(urls, local_names, method = &quot;libcurl&quot;, mode = &quot;wb&quot;) #for each file, load into /data In this particular case, we already know the file names. More often, we have downloaded a number of files, and know the file name format, but want to avoid typing in the names ourselves. The dir function does the job for us, and matches to a pattern. Best to check the number of files, or check the list by eye just to see that the pattern didn’t find files that you don’t want to process. We need something like the code for extract_locs, but automatic “recycling” does not work here. What we need is sometimes called a ‘Cartesian’ or a ‘cross’ join, where all combinations are generated. The latter is the clue to the function name. We will use tidyr::crossing to make all combinations of file name and skip (with n_rows along for the ride). SID_files &lt;- dir(&quot;data&quot;, pattern = &quot;SID_Monthly&quot;, full.names = TRUE) # force all combinations of file and skip extract_locs &lt;- data.frame(file = SID_files) %&gt;% crossing(data.frame(skip = seq(9, 199, by = 19), n_rows = 14)) # and load many_segment_monthlies &lt;- extract_locs %&gt;% pmap(extract_monthlies) %&gt;% bind_rows() It might take a minute to load the files, but it didn’t take much code. There’s a lot more that we can do with such data: tidying, summarising, adapting, plotting. But for that we need group_by, which is the main theme of the next chapter. In the meantime, save the file to data/ rather than rely on your project to keep a track of it. (We covered save at the end of section 3.1.) 6.3.4 Exercises 6.3.4.1 Questions Use lapply and an anonymous (temporary) function to loop over the skips. What happens if you unlist the result? Use read_xlsx to load the A-column from the file. Use seq to generate the row position of the airspace names (rather than the starts of the data). Hence extract the airspace names. Is the monthlies dataset tidy? Plot total flights with one line per airspace. How would you improve this graph? Where is the drop in flights resulting from the COVID-19 lockdowns? Try a single data.frame without the crossing. Why does R complain? As for question 4, but facet by market segment. From the SID, download a longer set of data, for multiple market segments. Adapt the code you have used to import this. Plot the data, for a quick visual check. Adapt extract_monthlies to allow the user to modify the column names. Advanced version: Use the ... when doing this, but why might this be risky? What sanity checking of column names would be useful? Advanced: Make an alternative version of combo_str_c which uses crossing instead of lapply. 6.3.4.2 Answers It seems to work, but you get a list of answers. Unlist just puts everything into a single vector, all structure is lost. There are ways to save this (eg bind_rows(monthlies) is how to bind this list of results together row-wise), but even then, we lose track of which file the data have come from. this leads towards an approach that becomes increasingly like a construction from Heath-Robinson (or Panamarenko, if you prefer). skips &lt;- seq(9, 199, by = 19) monthlies &lt;- lapply(skips, function(x) readxl::read_xlsx(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, skip = x, n_max = 14, col_names = monthly_cols)) temp &lt;- unlist(monthlies) Code like this. I followed through the links in the help file to find the cell_cols syntax, but you could also just use something like \"A1:A200\", knowing that you only need the first 197 anyway, and read_xlsx drops blank rows from the end. name_rows &lt;- seq(7, 197, by = 19) airspace_names &lt;- readxl::read_xlsx(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, range = cellranger::cell_cols(&quot;A&quot;), col_names = &quot;airspace&quot;) %&gt;% slice(name_rows) You could also use [] syntax instead of slice. Can you give two alternative approaches to this? Even if it has only 1 column, airspace_names is a dataframe, so you need to refer to both the column and the rows. # two approaches airspace_names &lt;- readxl::read_xlsx(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, range = cellranger::cell_cols(&quot;A&quot;), col_names = &quot;airspace&quot;) v1 &lt;- airspace_names$airspace[name_rows] v2 &lt;- airspace_names[name_rows, 1] # and a third - pull out the vector - in effect does the same as the &#39;$&#39; in 1. v3 &lt;- airspace_names %&gt;% pull(airspace) v3 &lt;- v3[name_rows] No, not tidy, because we have multiple columns of flight data. A minimum graph is something like this. ggplot(all_monthlies_v3, aes(month, T, colour = airspace)) + geom_line() + labs(y = &quot;Total flights in month&quot;) There are rather too many colours. Perhaps split into large and smaller groups and facet. Or one facet per airspace. ggplot formats the month axis nicely: probably labelling this as “month” is unnecessary, so remove it. Without the context of 2019, the drop in traffic is not evident. But in fact, this is the all-cargo market segment which was relatively unaffected by COVID-19, or at least did not see the deep declines in flights of other market segments. Because it’s not obvious how to recycle vectors of length 4, 19 and 1 to get the same lengths. A minimum graph is something like this. Three further experiments: free up the y-axis, between the four facets; transform T into thousands (as we did for millions in previous examples); remove ECAC, as it is quite dominant. ggplot(many_segment_monthlies, aes(month, T, colour = airspace)) + geom_line() + facet_wrap(~ segment) + labs(y = &quot;Total flights in month&quot;) If you save the downloaded Excel files into the data directory then you’ll need to name them so that you can tell them apart from the first batch. Easier might be to use a subdirectory of data. Otherwise, just adapt skip and n_max. We do the full monty and make all three parts adaptable. It’s polite, to yourself and others, when adding parameters to an existing function, to make the default behaviour exactly as it was before. That way, any existing uses of the function will continue to work. List new parameters after the old ones, and supply default values to give the original behaviour. extract_monthlies &lt;- function(file, skip, n_rows, col1 = c(&quot;A&quot;, &quot;D&quot;, &quot;I&quot;, &quot;O&quot;, &quot;T&quot;), col2 = c(&quot;_last&quot;, &quot;&quot;, &quot;_growth&quot;), sep = &quot;&quot;){ stopifnot(length(col1) == 5) stopifnot(length(col2) == 3) monthly_cols &lt;- c(&quot;month&quot;, combos_str_c(col1, col2, sep = sep)) meta_segment &lt;- readxl::read_xlsx(file, range = &quot;A2&quot;, col_names = &quot;segment&quot;) meta_units &lt;- readxl::read_xlsx(file, range = &quot;A4&quot;, col_names = &quot;units&quot;) # need to create an Excel reference &quot;An&quot; where n is just before skip meta_airspace &lt;- readxl::read_xlsx(file, range = str_c(&quot;A&quot;, skip - 2), col_names = &quot;airspace&quot;) monthlies &lt;- readxl::read_xlsx(file, skip = skip, n_max = n_rows, col_names = monthly_cols) # this is a quick way to return a dataframe reasonable column order # the first 3 get recycled as usual, to the same length as monthlies bind_cols(meta_airspace, meta_segment, meta_units, monthlies) } monthlies &lt;- extract_monthlies(&quot;data/SID_Monthly_AllCargo.xlsx&quot;, skip = 9, n_rows = 14, col1 = c(&quot;Arr&quot;, &quot;Dep&quot;, &quot;Int&quot;, &quot;Over&quot;, &quot;Total&quot;)) For the ‘advanced’ version, the clue is if you find yourself writing sep = sep to pass parameters on to another function. Here we replace both sep = \"\" and sep = sep with .... There is a risk with this that a user could then also pass the collapse parameter, which would have unfortunate consequences. So safer is not to use the ... here. It’s important that col1 has 5 values, and col2 has 3 values, otherwise the column naming will be scrambled. How do you get the number of values in a vector? With that, we also need the function stopifnot for checking parameters. Here we would add two lines, eg stopifnot(length(col1) == 5), stopifnot(length(col2) == 3) at the start of the function. Try this. We use two lines because then the error reporting is more precise: it tells the user exactly which parameter is the wrong length. crossing replaces lapply for us, but it returns a dataframe, and str_c is not be happy with a tibble or dataframe, it wants a vector. In section 4.3 we saw pull for getting a vector out of a dataframe. With this, we can make it work. It’s not clearly simpler than using lapply, but it’s more self explanatory, because, rather than seeing lapply which could be any sort of loop and you need to read it to work out why the loop is there, you’ve got crossing written clearly. alt_combos_str_c &lt;- function(s1, s2, ...){ # get all combinations, as a dataframe cross &lt;- crossing(s1, s2) # paste the vectors from this str_c(cross %&gt;% pull(1), cross %&gt;% pull(2), ...) } alt_combos_str_c(c(&quot;A319&quot;, &quot;A320&quot;, &quot;A321&quot;), c(&quot;ceo&quot;, &quot;neo&quot;), sep=&quot;&quot;) ## [1] &quot;A319ceo&quot; &quot;A319neo&quot; &quot;A320ceo&quot; &quot;A320neo&quot; &quot;A321ceo&quot; &quot;A321neo&quot; Even if you didn’t get this answer correct, there is a useful ‘error pattern’ here: where your code does not work first time because either (a) a function returns a type of data that you were not expecting or (b) a function is fussy about the type of data that you pass it as a parameter. The functions of R are really powerful, and you do a lot by stringing them together. Sometimes, however, it feels like all the effort is in getting the data types aligned from one function to the next. "],["groups.html", "Chapter 7 Looping with groups 7.1 Summarising 7.2 Dates 7.3 Adding variables", " Chapter 7 Looping with groups It feels like we have hardly started to investigate the lovely set of data from the SID. It would be natural to compare countries, to produce totals or min and max per country, to compare months, to give just three examples. This calls for a sort of looping: for each country or for each month, do x. In the tidyverse this is done with group_by and a host of supporting functions, as we’ll explore in this chapter. The patterns that we explore in this chapter are some of those that you’ll use most. Often, your main decision is whether you need to summarise (section 7.1) or to change and add variables, mutate in tidyverse-speak (section 7.3), or a bit of both. In this chapter, you’ll be introduced to: lapply, rep, function, pmin, pmax, is.list, unlist, read_xlsx, map, pmap, crossing 7.1 Summarising Summarising radically reduces the number of rows in your dataset. You will also lose variables in the process. We saw an example in section 3.1, where we summarised to get annual totals. For that monthly data, you can think of it row-wise, as “aggregate the rows so that I just have years”, or column-wise, as “get rid of the months”. These are essentially the same, and both are achieved with summarise and related functions. After a summarise all that is left are the variables created by summarise and the grouping variables.7 7.2 Dates 7.3 Adding variables There’s an exception to this rule, with geographic data using package sf that we’ll see in chapter 8.↩︎ "],["maps.html", "Chapter 8 Maps and geographic data 8.1 Spatial features 8.2 Using the R&amp;D Data Archive 8.3 Countries and regions 8.4 Airports 8.5 Routes 8.6 Density of routes 8.7 What has gone wrong?", " Chapter 8 Maps and geographic data [This chapter has been written out of sequence so might refer to patterns that have not yet been covered in earlier chapters.] You don’t get far in visualising patterns of air traffic without a map, whether that’s a map of airports, of routes between airports, or of regions: countries or, more air-traffic-focused, blocks of airspace. In this chapter we’ll build a set of tools for such things. In this chapter, you’ll be introduced to: spatial features, s2, rnaturalearthdata, R&amp;D Data Archive, geom_sf, %&lt;&gt;%, st_transform, CRS and more. You might need to install the package hexbin, which is used by ggplot2 for some of the tasks in this chapter. 8.1 Spatial features We choose to use ‘spatial features’ package sf as the main source of geographic functions since, of the several options, it feels most like an extension to the tidyverse. sf gives us, amongst other things: sfc: geographic columns for data tables and tibbles, to hold points, lines and polygons; sf: spatial features objects, which are data tables and tibbles that have a designated default ‘geometry’ sfc column; functions which access libraries of geographic operations, such as for distances, intersections or unions; use of geom_sf() in a ggplot to plot one of these columns. We’ll shortly see an example, but a ‘typical’ sf-enriched dataframe might be have one row per flight and an sfc column which contains the route flown by the flight, as a sequence of longitude-latitude points, but all folded up into a single cell per flight. If that sounds complicated, sf makes it pretty easy to use such data, and the gain of being able to ‘see’ the whole flight as one row in the data explorer is great. There is lots of good documentation for ‘spatial features’ and plenty of good questions on StackExchange to help you with problems. There’s even a ‘cheatsheet’. Here we focus on using sf with aviation data. The use of supporting libraries by sf can create complexity, for example requiring installation of extra code, such as GDAL (you’ll see GDAL and other libraries mentioned when you attach sf in the next code chunk). We’ll mostly stick to asking sf to use the s2 library, for a number of reasons: the code is available directly as the s2 package, so installation problems should be few; we can quickly translate between s2 and sf; and s2 works directly on the sphere. library(sf) ## Linking to GEOS 3.11.1, GDAL 3.5.3, PROJ 9.1.1; sf_use_s2() is TRUE sf::sf_use_s2(TRUE) library(s2) # here we use a number of functions directly from s2 library(rnaturalearthdata) # for country maps As usual, if these library calls cause errors, then use install.packages(c(\"sf\", \"rnaturalearthdata\")) to get the packages. What does that mean ‘works directly on the sphere’? The main difference between geographic data and the sorts of data that we’ve been plotting so far is that the two dimensions, longitude and latitude, are not points on a plane, but on a sphere. One approach to working with such data is to project (i.e. ‘convert’) all your data onto the plane by transforming with a ‘coordinate reference system’ (CRS), and then work on the plane, allowing for some of the distortion that is inevitable, eg straight lines might no longer be straight. With s2 we work on the sphere itself, and transform only when we want to plot. I like the cleanliness of keeping CRS for plotting. There are some costs: in particular, the World isn’t actually spherical, so there are some errors in distance and area calculations for example. If your application is sensitive to distance errors of 0.5%, then other functions can be used, but we won’t go into detail of those here. Switch sf to using s2 with sf_use_s2(TRUE) whenever you attach library(sf), as we did at the start of this chapter. Then sf will use s2 versions of functions whenever it can without further intervention from you. What does sf give us? Here’s an example, using rnaturalearthdata, a package that handily provides country maps. After converting the map data to sf with st_as_sf(), we have one row of a dataframe (an sf object is still a dataframe) per country, and all the map polygons in a single column. We illustrate the use of filter() on text fields, in a manner that you’ve seen before, and then with a ‘logical’ function that tests for intersection between geometry which contains the map polygons, and our Dublin_Bucharest line. # a line from Dublin to Athens Dublin_Bucharest &lt;- matrix(c(-6.27, 53.42, 26.1, 44.57), ncol = 2, byrow = TRUE) |&gt; st_linestring() |&gt; # convert the matrix of long-lat into an sf-&#39;linestring&#39; # tell sf that it&#39;s in long-lat, EPSG4326 is a long-lat coordinate reference system st_sfc(crs = 4326) # a map demo - countries110 comes with lots of data such as population europe_countries &lt;- rnaturalearthdata::countries110 |&gt; st_as_sf() |&gt; # convert to sf # some of the polygons in the map cause issues so we zoom in # using metadata in the &#39;rnaturalearthdata&#39; dataframe filter(admin != &quot;Russia&quot; &amp; continent == &quot;Europe&quot;) |&gt; # now do the intersection - working on the sphere # sparse = FALSE is required to get a full-length logical vector # rather than a vector of row ids # st_intersects is a yes-no question filter(st_intersects(geometry, Dublin_Bucharest, sparse = FALSE)) |&gt; # st_intersection asks for the intersection of the two mutate(segment = st_intersection(Dublin_Bucharest, geometry)) # simplest of plots treats long-lat like x-y ggplot(europe_countries) + geom_sf(aes(fill = 1000 * gdp_md_est/pop_est)) + geom_sf(aes(geometry = segment), colour = &quot;white&quot;) + labs(fill = &quot;GDP\\nPer Capita\\n($ 000)&quot;, caption = &quot;Simplified map omits many islands.&quot;) In this one example, we see two geographical functions for intersection, st_intersects() and st_intersection(). The first is a yes/no question used to filter out just the countries we need. The second creates the sequence of 3 line segments that actually intersect the countries, broken at the Irish Sea and English Channel. That’s probably of limited use here, since flights are continuous, but it’ll be more useful later. We see a line is defined by just two points (as it should be, speaking geometrically) but showing as a curve when plotted on a rectilinear longitude-latitude grid (ie one with horizontal latitude lines and vertical longitudes). This demonstrates the line being a ‘great circle’8, but see question (4) for a more complete demonstration. We cover country maps in more detail in section 8.3.1. 8.1.1 Map Exercises 8.1.1.1 Questions What does the map look like of those European countries not overflown? Which countries are within 100km of the route? Before converting to sf, what’s the structure of the data in countries110? Add the Dublin-Bucharest line directly, with + geom_sf(data = Dublin_Bucharest). What’s wrong and what do you think is the problem? 8.1.1.2 Answers Wrap a !( ) around the filter and see. Svalbard looks big in this plot, and the map opens up to show French Guiana. Three additional States. Checking through the s2 package documentation, we need s2_dwithin() (not s2_within), so use s2::s2_dwithin(Dublin_Bucharest, 100 * 1000), since distances are in metres. Run just the first line and we get a SpatialPolygonsDataFrame which is actually a data frame plus some other elements. It’s based on an alternative way of handling spatial data (package sp), which we won’t be using here mostly because it feels less ‘tidyverse’. The line is shown by ggplot as a straight line on the plot, because ggplot joins points with straight lines. At present, geom_sf() doesn’t really understand great circles. Indeed, if you’re sharp-eyed you might see that the ‘great circle’ already shown is actually made up of straight segments in each country. We’ll see how to handle this later, but for the moment the lesson is: sf understands great circles, ggplot less so. 8.2 Using the R&amp;D Data Archive At this point, we’ll start using data from the EUROCONTROL R&amp;D Data Archive which, at the time of writing, provides fine-grained data on some 18 million European flights between 2015 and 2020, with more data added each quarter. With a business or academic email address you can ask for free access to this. 8.2.1 Flight Summaries Rather than clog up your hard-disk with data, start by just downloading some flight summaries, say for March 2019. The image below shows a step in this process. Check and accept the terms and conditions and you’ll get your file. Downloading flight summary data for one quarter Save the data in your project /data folder, then the following code will work. You can leave the file with its .csv.gz extension; R can handle this. (The terms &amp; conditions mean that the data cannot be bundled with this book.) The ‘Flights’ file contains one row per flight, and gives a range of data about that flight, including departure and destination airports (ADEP, ADES), planned and actual times, aircraft type, aircraft operator etc. See the documentation ‘metadata’ for more details. The column names aren’t very R-friendly, though. So we use a function to read the file and rename the columns. It takes a little while to load nearly 800,000 flights. Times in air traffic management are usually in ‘universal time coordinated’ (UTC), so we specify the time zone as “UTC”. Check the documentation ‘metadata’ section 3 for the explanations of the column names. get_flights &lt;- function(file_name){ # load Flights...csv.gz file downloaded from R&amp;D Data Archive readr::read_csv(file_name, skip = 1, col_names = c(&quot;id&quot;, &quot;adep&quot;, &quot;adep_lat&quot;, &quot;adep_long&quot;, &quot;ades&quot;, &quot;ades_lat&quot;, &quot;ades_long&quot;, &quot;obt_filed&quot;, &quot;arr_filed&quot;, &quot;obt_actual&quot;, &quot;arr_actual&quot;, &quot;ac_type&quot;, &quot;ao&quot;, &quot;ac_reg&quot;, &quot;flt_type&quot;, &quot;segment&quot;, &quot;rfl&quot;, &quot;dist_act_nm&quot;)) |&gt; mutate_at(vars(&quot;obt_filed&quot;, &quot;arr_filed&quot;, &quot;obt_actual&quot;, &quot;arr_actual&quot;), ~ as.POSIXct(., format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;UTC&quot;)) |&gt; mutate(dist_act_km = 1.852 * dist_act_nm) |&gt; select(-dist_act_nm) } flights &lt;- get_flights(&quot;data/Flights_20190301_20190331.csv.gz&quot;) It’s good to do a visual check of numbers to see what we have. We take the day part of the filed off-blocks time (the time in the flight plan for push-back from the gate). We saw previously that this needs to be floor_date not round_date to get the start of the day. There are about 25,000 flights per day, although at this time of year Saturdays are significantly quieter. The plot shows that we have a sensible number of flights, for each day in March. ggplot(flights, aes(lubridate::floor_date(obt_filed, &quot;day&quot;), fill = segment)) + geom_bar() + labs(x = &quot;Date (of filed off-blocks time)&quot;, y = &quot;Number of Commercial Flights in Sample&quot;) A check like this can be useful just to know how much data there is to explore. Also, although we haven’t joined these data yet to anything else, it’s surprising how often a simple join can go wrong and duplicate flights. So checks are worthwhile, and this gives us a baseline to re-check later. 8.2.2 Airspace structure The structure of the airspace is continually refined to align with traffic needs. In Europe, there are updates for each new ‘AIRAC cycle’, which lasts 4 weeks. The R&amp;D Data Archive provides airspace data for each applicable AIRAC cycle, and the smallest file in each quarter’s data is AIRAC_xxxx.csv.gz which lists the AIRAC cycles and their dates during that month. The airspace structure is then given in terms of: the boundaries and vertical extent of each flight information region (FIR) structure (with a simplified structure outside Europe). the route network. Many countries have a single ‘FIR’, though some share one (e.g. Belgium &amp; Luxembourg) and some have more than one (e.g. Spain). For coastal countries, the airspace and therefore the FIR usually extends out to sea. In this section we load and inspect the FIR data and compare it with the land boundaries. From the 201903 archive, download the FIR_1904.csv.gz file and put it in your data folder. We need to do a little work with the data. In the country data we saw briefly earlier, countries are described with ‘multipolygons’, each polygon being a sequence of points that joins up to form a ring and the ‘multi’ bit because there can be islands or, occasionally, holes (Italy has 2 holes, for the Vatican and San Marino). FIRs are no different. Most are single polygons, one or two have holes, and some have been split in these data along the dateline. We have to identify the rings. The way we have chosen to do this is by looking for repeated coordinates (which are the two ends (n() &gt; 1)), then finding the first end and flagging it with a 1. A cumulative sum of these flags gives distinct identification numbers to each ring. One respect in which FIRs are different from countries is that they have a vertical extent and can overlap if you ignore altitude. We treat combinations of airspace_id, min_flight_level, max_flight_level (see group_by) as determining an FIR. That isn’t really true (an FIR can be thick in places and thin in others), but this will be enough for this illustration. fir &lt;- readr::read_csv(&quot;data/FIR_1904.csv.gz&quot;) # tidy the column names # this uses the &#39;and assign&#39; pipe %&lt;&gt;%, for brevity colnames(fir) %&lt;&gt;% stringr::str_replace_all(&quot;\\\\s&quot;, &quot;_&quot;) %&lt;&gt;% stringr::str_to_lower() fir_poly &lt;- fir |&gt; group_by(airspace_id, min_flight_level, max_flight_level, longitude, latitude) |&gt; mutate(ring_end = (n() &gt; 1)) |&gt; # end points occur twice ungroup(longitude, latitude) |&gt; mutate(ring_start = if_else(ring_end &amp; (is.na(lag(ring_end)) | lag(ring_end)), 1L, 0L), ring_id = cumsum(ring_start)) |&gt; select(-ring_end, -ring_start) |&gt; # now make the polygon (with holes if that makes sense) summarise(geo = s2::s2_make_polygon(longitude, latitude, ring_id = ring_id) |&gt; sf::st_as_sfc()) |&gt; # turn into sf object sf::st_set_geometry(&quot;geo&quot;) |&gt; ungroup() A quick map highlights a number of features of the data: we just have large regions in places distant from Europe, these are aggregate regions made up of lots of actual FIRs; the ‘layers’ of superimposed FIRs are less transparent so mask the underlying countries more, eg Spain, though this isn’t a great way to show this; being Europe-oriented the data have been cut at 180º, to avoid wrapping problems (in fact the map is cut at 180° and the FIRs at 179°). ggplot(rnaturalearthdata::countries110 |&gt; st_as_sf()) + #convert to sf temporarily geom_sf(fill = &quot;grey50&quot;) + geom_sf(data = fir_poly, alpha = 0.7) This isn’t very pretty. We’ll look at a better version in section 8.3.2. 8.2.3 Flight profiles The R&amp;D Data Archive provides both the route from the filed flight plan, and the route as flown, updated by radar. Each route is specified as longitude-latitude points, a flight level (roughly of 100 feet, so FL350 = 35,000 feet altitude), and a time. We will load and plot some profiles in section 8.5.3. 8.2.4 Exercises 8.2.4.1 Questions In the bar chart of flight counts, why the fussy caption on the x-axis ‘of filed off-blocks time’? Download a second flight file. How would you automatically load and merge all of the flight files that you’ve downloaded. (Hint: This is a simplified version of something you did with files from the STATFOR dashboard in section 6.3.3. Why did the answer to (2) involve switching from pmap? 8.2.4.2 Answers Because flights might depart on one day and land the next. This is it. #get file names in the data directory, with the full file path flight_files &lt;- dir(&quot;data&quot;, pattern = &quot;Flights_&quot;, full.names = TRUE) # and load all_flights &lt;- flight_files |&gt; map(get_flights) |&gt; # this re-use is why we wrote get_flights as a function, earlier in this section bind_rows() Because the ‘SID’ code in the earlier chapter is reading a number of columns from a dataframe it uses pmap. Here we just have a single vector of names, so only need map. 8.3 Countries and regions In this section, we introduce work patterns for plotting countries or regions. We stick to the over-arching pattern of: keep the data in longitude-latitude (“on the sphere”), then transform when plotting into a coordinate reference system (CRS) chosen to suit the map. 8.3.1 Countries We already saw a country map in section 8.1. Let’s take a step back and approach some of the key elements of plotting countries at a more steady pace: filtering, plotting, transforming, cropping. It might be that you have a specific set of countries in mind. Then it’s natural to do this by filtering the dataframe as you would with a non-geographic dataframe. While you might filter by name, it’s more compact and often easier (due to alternative spellings of country names) to use the ISO code. The 1:110 million scale map in countries110, however, isn’t great when we zoom in to this detail (for example it misses out many islands), so we use a 1:50 million map countries50. An even finer-grained, 1:10 million map is available, but not through CRAN, in the package rnaturalearthhires, while other European maps are available for free download, for example from Eurostat. While we filter on the code iso_a2 for the reasons discussed earlier, we use the admin field for the colour fill, since this is an easy way to create a more informative legend, one that shows the administrative names. iberia_iso2 &lt;- c(&quot;ES&quot;, &quot;PT&quot;) #Spain &amp; Portugal iberia &lt;- rnaturalearthdata::countries50 |&gt; st_as_sf() |&gt; filter(iso_a2 %in% iberia_iso2) # colour by country, make the borders thin. ggplot(iberia) + geom_sf(aes(fill = admin), size = 0.1) + labs(fill = &quot;Country&quot;) + theme_minimal() There’s something a little ugly about this map - the shapes are distorted because we aren’t doing any sort of projection. We are just plotting longitude and latitude as if they were x-y coordinates, which they aren’t. We could transform the data (the sf or the geometry column) to a new CRS, but because we’re working with s2, on the sphere in long-lat, it makes sense to keep our data in long-lat, and transform only for the purpose of plotting. The easiest way to do that is to add a coord_sf(crs = xx) to the plot, which ensures that all of the layers (if we have more than one layer) are in the same CRS. For European maps, EPSG3035 is often recommended for statistical charts: it’s a Lambert Equal Area transformation set up for Europe. So here we just add coord_sf(crs = 3035). use_crs &lt;- 3035 # use ggplot::last_plot() rather than repeat the code last_plot() + # apply the transform coord_sf(crs = use_crs) What if we want a map without Canaries or Azores, but with Madeira, say? With some sets of maps, the parts of countries are separate so can be filtered by name. In the rnaturalearth dataframe they are not. We need to crop the map. Again, this is about the map, not the data, so rather than cropping the data, we basically need to set limits to the plot. The catch is that we need to do this in the units of the projection (EPSG3035) which, after the transformation, is metres (measured from a projection-specific origin). To make the code easier to understand, we probably want to crop in terms of degrees. So we create some points (in degrees) and transform them, too, to get the values in metres. In degrees, think in terms of what point should be in the bottom left corner (here 20 West 30 North), and what in the top right. We’ll shortly see how to crop automatically based on the data, but it’s useful to know the bottom-left/top-right rule. #set limits at long-lat (-20, 30) to (5, 45) lim_corners &lt;- matrix(c(-20, 30, 5, 45), ncol = 2, byrow = TRUE) |&gt; st_multipoint() |&gt; # convert to sf multiple-points st_sfc(crs = 4326) |&gt; # and record that these are points in long-lat # transform to the map CRS st_transform(use_crs) |&gt; # pull out a matrix of values (with named columns X and Y) st_coordinates() # the ggplot in full, rather than use ggplot2::last_plot() ggplot(iberia) + geom_sf(aes(fill = admin), size = 0.1) + labs(fill = &quot;Country&quot;) + theme_minimal() + # apply the crop coord_sf(xlim = c(lim_corners[1, &quot;X&quot;], lim_corners[2, &quot;X&quot;]), ylim = c(lim_corners[1, &quot;Y&quot;], lim_corners[2, &quot;Y&quot;]), crs = use_crs) That last chunk uses ‘crs’ in 3 different ways. The first two are easy to confuse: |&gt; st_sfc(crs = ) means ‘note that the data just given are in terms of this crs’: it adds an attribute to the multipoint, without changing the values. Without this, sf doesn’t know if we’ve specified the box in degrees, or metres measured from the Equator or something else. |&gt; st_transform(use_crs) says ‘convert from the current crs to use_crs’. It actually changes the data. coord_sf(... crs = use_crs) tells ggplot to plot using the CRS use_crs. These steps are fiddly, but a convenient way to convert a required bounding box with an edge at 5East from longitude (easy to enter in code) to CRS3035 units lim_corners[2, \"Y\"] is 2.4452488*106 (in metres, so about 2,400km from somewhere). 8.3.2 Airspace In section 8.2.2 we made a quick check of the FIRs with a map. Here we make a tidier map. When plotting the whole World, the default map projection makes even worse distortions than we saw for Iberia. We would like to use a better representation of the area of countries, but need to handle the problem seen in the exercises in section 8.1: that ggplot just draws straight lines. In the R&amp;D data, some of the FIRs distant from Europe are described with a minimum of points, such as a straight line from (179, 0) to (179, 90). We solve this problem by adding extra points at an arbitrary 1° interval using st_segmentize(). This example illustrates a strength of an sf dataframe. It can contain many sfc columns, but one is the designated geometry column. We can then apply a function such as st_segmentize to the whole dataframe and it will pick out the geometry column to work with, where that’s appropriate. The Lambert (EPSG3035) projection also distorts on the whole-World scale. Instead, we choose an Atlantic-centred Robinson projection, creating it from a standardised text string using sp::CRS. XKCD suggests this reveals something of my age and indeed, it’s a familiar projection from school atlases. This CRS is also available as himach::crs_Atlantic, so if you have that package installed you can save some typing for the first line of the next chunk. And finally we colour code the upper flight level of the FIR - so the darker blue areas are lower FIRs. crs_robinson &lt;- sp::CRS(&quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs +towgs84=0,0,0&quot;) ggplot(rnaturalearthdata::countries110 |&gt; st_as_sf()) + geom_sf(fill = &quot;grey50&quot;) + geom_sf(data = fir_poly |&gt; st_segmentize(units::set_units(1, degree)), # just for plotting add intermediate points aes(fill = max_flight_level), alpha = 0.6) + labs(fill = &quot;Upper Flight Level of FIR&quot;) + coord_sf(crs = crs_robinson) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) If we want to zoom in to Europe, which is where most of the flight data is, it is tempting to filter on the FIR airspace ID, though this harder than just picking those starting with ‘E’, ‘L’ or ‘B’, eg what about the Canary Islands, or Ukraine? It is probably best to avoid this approach unless you have a very specific list of countries to include. As in the previous section, instead we set the limits of the plot using the ‘bottom-left, top-right and transform’ method. use_crs &lt;- crs_robinson #set limits at long-lat (-20, 25) to (70, 80) lim_corners &lt;- st_multipoint(matrix(c(-20, 25, 70, 80), ncol = 2, byrow = 2)) |&gt; st_sfc(crs = 4326) |&gt; #transform then pull out a matrix of values (with named columns X and Y) st_transform(use_crs) |&gt; st_coordinates() # [exercise: can you use last_plot() to save some typing here?] ggplot(rnaturalearthdata::countries50 |&gt; st_as_sf()) + geom_sf(fill = &quot;grey50&quot;) + # background colour for the land geom_sf(data = fir_poly |&gt; st_segmentize(units::set_units(1, degree)), aes(fill = max_flight_level), alpha = 0.6) + labs(fill = &quot;Upper\\nFlight Level\\nof FIR&quot;) + coord_sf(crs = use_crs, xlim = c(lim_corners[1, &quot;X&quot;], lim_corners[2, &quot;X&quot;]), ylim = c(lim_corners[1, &quot;Y&quot;], lim_corners[2, &quot;Y&quot;])) + theme_minimal() As a plot of a 3D-layered structure this still misses something, but it’s not bad to get an idea of the airspace structure and where there are layers. What about labels? For this we just need to define the label text, which we do by extracting just the 4-letter FIR codes, excluding the “FIR” and “UIR”. Chapter 9 has much more on patterns such as \"FIR|UIR\". The geom_sf_text will be put at the centroid of the polygon by default. We’ll see how to deconflict labels on maps in section 8.4.1. fir_poly &lt;- fir_poly |&gt; mutate(airspace = stringr::str_remove(airspace_id, &quot;(UIR|FIR)&quot;)) |&gt; group_by(airspace) |&gt; mutate(label = if_else(max_flight_level == max(max_flight_level), airspace, &quot;&quot;)) ggplot(fir_poly |&gt; st_segmentize(units::set_units(1, degree))) + geom_sf(data = rnaturalearthdata::countries50 |&gt; st_as_sf(), fill = &quot;grey50&quot;) + # background colour for the land geom_sf(alpha = 0.6) + geom_sf_text(aes(label = label), size = 1.8) + coord_sf(crs = use_crs, xlim = c(lim_corners[1, &quot;X&quot;], lim_corners[2, &quot;X&quot;]), ylim = c(lim_corners[1, &quot;Y&quot;], lim_corners[2, &quot;Y&quot;])) + theme_minimal() 8.3.3 Exercises 8.3.3.1 Questions In the final map, adapt the code so that it will plot a slice at a particular flight level, and use an appropriate title. One messy part of the FIR map is GCCC, where, after taking away the FIR/UIR we have overlapping labels for GCCC and GCCCN. How could we make them the same (which would look tidier, at the expense of precision)? In the two versions of the code for the FIR map, why do we swap where fir_poly and countries50 are mentioned (ggplot and geom_sf)? 8.3.3.2 Answers We can filter, and then use the original label. So one solution is as follows. at_fl &lt;- 350 fir_slice &lt;- fir_poly |&gt; # max of lower = min of upper, so we arbitrarily put the boundary in the lower filter(min_flight_level &lt; at_fl &amp; at_fl &lt;= max_flight_level) ggplot(fir_slice |&gt; st_segmentize(units::set_units(1, degree))) + geom_sf(data = rnaturalearthdata::countries50 |&gt; st_as_sf(), fill = &quot;grey50&quot;) + # background colour for the land geom_sf(alpha = 0.6) + geom_sf_text(aes(label = airspace_id), size = 1.8) + labs(x = &quot;&quot;, y = &quot;&quot;, title = stringr::str_c(&quot;FIR/UIRs at flight level &quot;, at_fl)) + coord_sf(crs = use_crs, xlim = c(lim_corners[1, &quot;X&quot;], lim_corners[2, &quot;X&quot;]), ylim = c(lim_corners[1, &quot;Y&quot;], lim_corners[2, &quot;Y&quot;])) + theme_minimal() A pattern which hides an optional “S” or “N” would be “(UIR|FIR)(S|N)?”. In the first map it didn’t really matter, but the countries are the first layer so they were put first. In the second, we want the fir_poly to be the default data for the additional geom_sf_text, so it has to be the one inside the ggplot. The country data is still plotted first (by the first geom_sf()). 8.4 Airports In this section we look at plotting airports. Much of it is assembling techniques we’ve already seen: plotting points with geom_point in chapter 3 and elsewhere; map layers from earlier in this chapter. New ideas will include: preferring to plot densities rather than simple counts; and how to properly de-conflict labels on maps (rather than the bodged answer to question 3!). As a warm up, let’s plot the busiest airports for one market segment; use unique(flights$segment) to get a list of all market segments that are available in the data. Still scope for something new: we see a quick way to convert two columns (longitude, latitude) into a spatial feature column, another use of st_as_sf(); and we work out the bounding box automatically, using st_bbox() and also st_buffer() so as not to have airports too close to the edge of the map. # to avoid repeating ourselves with the background map geom_sf_bg &lt;- function( ..., m = rnaturalearthdata::countries50, fill = &quot;grey80&quot;, colour = &quot;white&quot;, size = 0.1) { geom_sf(..., data = m |&gt; st_as_sfc(), fill = fill, colour = colour, size = size) } # and to get bounding box for an sf, in plot coordinates get_bounds &lt;- function(sft, crs = 3035, margin_km = 100){ sft |&gt; st_transform(crs) |&gt; # convert to metres (TBD check if units are metres) st_buffer(margin_km * 1000) |&gt; # add the buffer st_bbox() # get the bounding box for the selected airports } crs_europe &lt;- 3035 #EPSG3035 is our good CRS for Europe maps busiest_n &lt;- 20 show_segment &lt;- &quot;Business Aviation&quot; busy_ap &lt;- flights |&gt; filter(segment == show_segment) |&gt; # the data are one row per flight, so we need to count rows # keep just 3 columns and sort largest to smallest count(adep, adep_lat, adep_long, sort = TRUE) |&gt; slice(1:busiest_n) |&gt; # convert to sf, noting that long and lat are in degrees (EPSG4326) st_as_sf(coords = c(&quot;adep_long&quot;, &quot;adep_lat&quot;), crs = 4326) bounds &lt;- get_bounds(busy_ap) ggplot(busy_ap) + geom_sf_bg() + #default background map geom_sf(aes(size = n), alpha = 0.6) + labs(size = &quot;Number of\\ndepartures&quot;, title = stringr::str_c(show_segment, &quot; in March 2019.&quot;), caption = stringr::str_c(&quot;Top &quot;, busiest_n, &quot; airports are shown.&quot;)) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax)) + theme_minimal() 8.4.1 Overlapping labels on maps We could label the maps with the airport codes in the same way that we did with the FIRs [try this], but the labels are overlapping. What function did we use in chapter 4 to solve this problem? [try this, too] The problem is that ggrepel::geom_text_repel() doesn’t immediately understand the geometry column in our busy_ap data. Happily, a little googling comes to our aid: there is a way to get geom_text_repel still to do the work for us and convert what we do have, a geometry, into the x and y that it needs. All it takes is to use the additional parameter stat = \"sf_coordinates\". last_plot() + # assumes you didn&#39;t actually do the 2 little exercises in [ ] ggrepel::geom_text_repel(aes(geometry = geometry, label = adep), stat = &quot;sf_coordinates&quot;, size = 1.8) 8.5 Routes The routes of aircraft are typically shown as either: a great circle joining the departure and destination airports; or a sequence of short line segments (which should each be segments of great circles) joining points along the actual route flown, where the points come from radar or other surveillance. In this section we will see examples of each of these. 8.5.1 Great Circle Routes We saw in section 8.1 that geom_sf doesn’t give us great circles by default, but joins with a straight line. There are two ways to get around this: use a CRS in which great circles map to straight lines, and our go-to CRS, EPSG3035, for Europe more-or-less has this nice property, as does the Lambert Conformal Conic (such as EPSG9040 for Europe) that is often used for aeronautical maps; alternatively, add intermediate points before plotting. Which you choose depends on how important the routing is. A great-circle is often already ‘schematic’: just a way to link airports for the viewer of the map, rather than being too fussy about the intermediate points. But if you think the viewer will jump to conclusions about traffic density in the airspace, then best to use something more precise. In the first example, we pick low-cost routes, which are mostly short- or medium-haul because this keeps the map mostly in Europe and therefore with less distortion (though Cuba in this map looks like it’s been on a diet, not to mention being rotated to a confusing angle). We use the Lambert equal area projection, which we saved as crs_europe and take the simpler approach of plotting straight lines. We stick to the principle of keeping the data in longitude-latitude, and only transform when plotting into a coordinate reference system (CRS). But there’s still some manipulation of the data to do: for each row (rowwise() is a particular sort of group_by that works row by row) we have four separate columns with the start and end coordinates. From these we create a line, and make it suitable for use in a dataframe (st_sfc) and tell sf that the CRS is long-lat (crs=4326). We’ll see a faster way to do this in section 8.5.2. set.seed(380) # to get a fixed, but random sample lcc_sample &lt;- flights |&gt; # just short- or medium-haul lowcost filter(segment == &quot;Lowcost&quot;) |&gt; slice_sample(n = 100) |&gt; #just pick a few for this example rowwise() |&gt; # we want to have one line per row of data mutate(gc = st_linestring(matrix(c(adep_long, ades_long, adep_lat, ades_lat), ncol = 2)) |&gt; st_sfc(crs = 4326)) |&gt; ungroup() |&gt; st_set_geometry(&quot;gc&quot;) #bounds doesn&#39;t mind if the sf geometry is points, lines, ... bounds &lt;- get_bounds(lcc_sample) ggplot(lcc_sample) + geom_sf_bg() + geom_sf(alpha = 0.3) + labs(title = &quot;Sample of Low Cost routes&quot;) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax)) + theme_minimal() In the second example, we use a sample of longer-haul routes and the Robinson CRS. Construction of the data is the same, apart from the filter. But we need to specify a non-default crs in our bounds function. The work of adding extra points to each line is done with the st_segmentize, which conveniently we can apply to the whole data frame (as mentioned, it picks out the geometry column automatically for segmentation). You can adapt the step of the segments in degrees or km, appropriate to your target map. More looks nicer, but it takes up space, and takes time to plot. set.seed(777) # to get a fixed, but random sample long_sample &lt;- flights |&gt; filter(dist_act_km &gt; 4000) |&gt; slice_sample(n = 100) |&gt; #just pick a few for this example rowwise() |&gt; # we want to have one line per row of data mutate(gc = st_linestring(matrix(c(adep_long, ades_long, adep_lat, ades_lat), ncol = 2)) |&gt; st_sfc(crs = 4326)) |&gt; # these are longitude-latitude pairs ungroup() |&gt; st_set_geometry(&quot;gc&quot;) #bounds doesn&#39;t mind if the sf geometry is points, lines, ... bounds &lt;- get_bounds(long_sample, crs = crs_robinson) ggplot(long_sample |&gt; st_segmentize(units::set_units(1, degree)) ) + # add points temporarily, for plotting geom_sf_bg() + geom_sf(alpha = 0.3) + labs(title = &quot;Sample of long-haul routes&quot;) + coord_sf(crs = crs_robinson, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax)) + theme_minimal() 8.5.2 Need for Speed [This is an advanced section that uses quickly some packages that have not been introduced. But the gains in speed are considerable.] We’ve been using small samples of data so far. Any larger and there will be a noticeable delay in converting from long-lat to spatial features, unless you’re very lucky in your hardware. In this section we discuss two ways to speed things up: an easy one, and a fiddly one. The easy speed-up is to avoid geo-coding the same route twice: the same airport pair is likely to occur several times in a large dataset: there are 767 occurrences of Madrid-Barcelona (LEMD-LEBL) for example. If we’re only looking at great circle, then all of these routes are the same. Even with filtering and grouping for the statistics you’re calculating (by aircraft type, by market segment etc), there’s still going to be duplication, unless you really need the precise time (which is unlikely since it’s tricky to show on a map). So best to group_by the variables you need to keep (including adep_long, ades_long, adep_lat, ades_lat) and summarise (see section 7.1), taking a flight count [what code would that use?]. Only then convert to sf with st_linestring and friends, as in the previous section. The fiddly method is less obvious because it depends on developing a feel for what works quickly and what less quickly in R. To my mind, using rowwise feels like I’ve failed. I might not have written for...next, but in effect, I’ve fallen back onto a for-next loop over each row in turn. I’m left believing I’ve missed a neat tidyverse solution, even if I’m being unfair and there might not be a neater alternative. The rowwise we used earlier is an example where there is a more efficient way to code. This isn’t just linguistic prejudice against for...next. Calling a function, here st_linestring, takes time, so calling it for each of 800,000 rows one-by-one in the flight data is going to be time-consuming. There is an interesting discussion of the conversion to great circles here. The solution is based on the purrr package that we saw in chapter 6. Instead of grouping (each row separately), it creates a list of lists - one list per row and that list contains the 4 longitude-latitude numbers. We saw map in chapter 6 as an efficient way to operate on each element of a list. In this code chunk, we set up two parallel ways of coding the same thing. The identical test shows that the two results are the same. The microbenchmark shows, over 10 iterations of each, that the purrr approach is almost 15 times faster. library(microbenchmark) flts &lt;- set.seed(777) # to get a fixed, but random sample long_sample &lt;- flights |&gt; filter(dist_act_km &gt; 4000) |&gt; slice_sample(n = 100) # adapted from Charlie Hadley https://www.findingyourway.io/blog/2018/02/28/2018-02-28_great-circles-with-sf-and-leaflet/ # since his example was written, the {{ }} syntax has become available for referring to dataframe columns # so we see {{ }} here in place of his `enquo()` # this assumes df is not already an `sf` object, just a dataframe or tbl. longlat_to_sf &lt;- function(df, start_long = adep_long, start_lat = adep_lat, end_long = ades_long, end_lat = ades_lat) { df %&gt;% # {{ start_long }} selects the column referred to by the value of start_long (without quotes) select( {{ start_long }}, {{ end_long }}, {{ start_lat }}, {{ end_lat }} ) %&gt;% # make a list of lists, each of 4 numbers transpose() %&gt;% # turn each list into a matrix map(~ matrix(flatten_dbl(.), nrow = 2)) %&gt;% # feed that into st_linestring map(st_linestring) %&gt;% st_sfc(crs = 4326) %&gt;% st_sf(geometry = .) %&gt;% rename(gc = geometry) %&gt;% bind_cols(df) %&gt;% relocate(gc, .after = {{ end_long }}) } # compare the geography data that results - identical = TRUE identical( long_sample |&gt; rowwise() |&gt; # we want to have one line per row of data mutate(gc = st_linestring(matrix(c(adep_long, ades_long, adep_lat, ades_lat), ncol = 2)) |&gt; st_sfc(crs = 4326)) |&gt; ungroup() |&gt; pull(gc), long_sample |&gt; longlat_to_sf()|&gt; pull(gc) ) ## [1] TRUE # is it faster? microbenchmark::microbenchmark( rowwise = long_sample |&gt; rowwise() |&gt; # we want to have one line per row of data mutate(gc = st_linestring(matrix(c(adep_long, ades_long, adep_lat, ades_lat), ncol = 2)) |&gt; st_sfc(crs = 4326)) |&gt; ungroup(), purrr = long_sample |&gt; longlat_to_sf(), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval ## rowwise 176.16844 178.17314 186.68337 184.48668 195.66480 203.31266 10 ## purrr 13.02097 13.39099 15.03075 13.96798 14.28085 26.47409 10 The longlat_to_sf function is hard to follow. But it works. Don’t worry too much how unless you’re really interested. Just adopt the long_sample |&gt; longlat_to_sf() syntax. If your longitudes and latitudes are in columns with different names, say ‘long1’, ‘lat1’, you just use longlat_to_sf(start_long = long1, start_lat = lat1, and so on...) 8.5.3 True Routes Now is the time to dip our toes into the route data from the R&amp;D Data Archive. The files are large, so handle with care, but download Flight_Points_Actual_20190301_20190331.csv.gz into your /data directory, as you did the other files. This one has some 26 million rows, so we’ll create a large sample to play with, flights by the turboprop aircraft AT72. In chapter [TBD] we’ll see how to handle these datasets faster using datatables. ac &lt;- &quot;AT72&quot; # an aircraft type # a sizeable sample ac_flts &lt;- flights |&gt; filter(ac_type == ac) # get the data routes &lt;- readr::read_csv(&quot;data/Flight_Points_Actual_20190301_20190331.csv.gz&quot;, skip = 1, col_names = c(&quot;id&quot;, &quot;seq&quot;, &quot;time&quot;, &quot;level&quot;, &quot;lat&quot;, &quot;long&quot;)) |&gt; mutate(time = as.POSIXct(time, format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;UTC&quot;)) routes %&lt;&gt;% filter(id %in% ac_flts$id) # reduce to a sample save(routes, file = stringr::str_c(&quot;data/Flight_Points_Actual_201903_&quot;, ac, &quot;.rda&quot;)) Let’s explore these data. Firstly by just plotting the points in the data and their flight level, we get a rapid picture of the airports from which these ATR turboprop aircraft operate. load(&quot;data/Flight_Points_Actual_201903_AT72.rda&quot;) # loads routes dataset # convert in two steps route_pts &lt;- routes |&gt; drop_na() |&gt; st_as_sf(coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) bounds &lt;- get_bounds(route_pts) ggplot(route_pts) + geom_sf_bg() + geom_sf(aes(colour = level), size = 0.1, alpha = 0.7) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax)) + theme_minimal() + labs(title = &quot;Actual routes flown by AT72&quot;, colour = &quot;Flight level&quot;, x=&quot;&quot;, y=&quot;&quot;) + scale_colour_viridis_b(direction = -1) This map gives a feel for the frequency at which the actual route is reported. It also shows some slightly odd-looking routing over the Atlantic (where radar coverage can be lacking). For a more joined-up view, we need to convert the points into lines. The ‘normal’ approach to this is to summarise and then st_cast. But why does this work? Surely after summarise you only have group variables and variables created in the call (time and level in the example below)? Normally this is true. But the geometry column in an sf dataframe is ‘sticky’, and trumps this normal behaviour: summarising an sf silently summarises the geometry column into something sensible. In this case we get a ‘multipoint’ (a set of points) which we can then cast into a ‘linestring’ (a set of points joined into a single line). So the overall sequence is: st_as_sf to get points, group, summarise, cast. There’s one small twist: summarise normally involves a union operation which (unusually for R) changes the data order. So to keep the points in order we need to tell summarise to skip the union step. load(&quot;data/Flight_Points_Actual_201903_AT72.rda&quot;) # convert in two steps geo_routes &lt;- routes |&gt; drop_na() |&gt; #use a 4D point st_as_sf(coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) |&gt; # then &#39;summarise&#39; into a linestring group_by(id) |&gt; summarise(time = first(time), level = max(level), do_union = FALSE) |&gt; st_cast(&quot;LINESTRING&quot;) |&gt; # for this scale of map not really necessary, but segmentize anyway st_segmentize(units::set_units(0.5, degree)) bounds &lt;- get_bounds(geo_routes) # use our bounds-finding function ggplot(geo_routes) + geom_sf_bg() + # and our default land background, see earlier geom_sf(aes(colour = level), size = 0.2, alpha = 0.7) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax)) + theme_minimal() + labs(title = &quot;Actual routes flown by AT72&quot;, colour = &quot;Max\\nflight level&quot;, x=&quot;&quot;, y=&quot;&quot;) + scale_colour_viridis_b(direction = -1) 8.5.4 Exercises 8.5.4.1 Questions Experiment with different colours, line thickness and transparency in the low-cost map. In the low-cost map, take a larger sample of flights up to 1500km, and count the flights per airport pair. Colour the lines by traffic. Once ready, drop the sample completely, to give full counts for the month. Beware, some longitude or latitude might be missing, and such airport pairs need to be dropped. Add labels for the top N (eg 20) airports in this extended low-cost map. In the long-haul route map, why do some of the routes leave the map? What’s the solution to this? When reducing routes to those for AT72 aircraft (start of section 8.5.3), we used a filter. With a big dataset, finding efficient code can save a lot of time. Is filter quicker or slower than using an inner_join? 8.5.4.2 Answers Using colour, size and alpha parameters. Something like this. We needed to drop airports with missing long-lat, using drop_na. It took a little work on the colouring, and line sizes to get something presentable. That includes sorting so that the busier routes get plotted last. lcc_counts &lt;- flights |&gt; # just short- or medium-haul lowcost filter(segment == &quot;Lowcost&quot; &amp; dist_act_km &lt;= 1500) |&gt; drop_na() |&gt; # can&#39;t plot if we don&#39;t know the lat long of an AP # slice_sample(n = 1000) |&gt; #just pick a few while debugging # count is a short-hand for group/summarise/ungroup count(adep, ades, adep_long, ades_long, adep_lat, ades_lat, name = &quot;flts&quot;) |&gt; rowwise() |&gt; # we want to have one line per row of data mutate(gc = st_linestring(matrix(c(adep_long, ades_long, adep_lat, ades_lat), ncol = 2)) |&gt; st_sfc(crs = 4326)) |&gt; ungroup() |&gt; arrange(flts) |&gt; st_set_geometry(&quot;gc&quot;) #bounds doesn&#39;t mind if the sf geometry is points, lines, ... bounds &lt;- get_bounds(lcc_counts) ggplot(lcc_counts) + geom_sf_bg() + geom_sf(aes(colour = flts), alpha = 0.4, size = 0.2) + labs(title = &quot;Low-cost routes&quot;, colour = &quot;Flights\\nin the month&quot;) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax)) + theme_minimal() + scale_colour_viridis_b(direction = -1) See the extended exercises in chapter 5 for an example of selecting top n and plotting deconflicted labels. 8.4.1 will also be helpful. We’re applying bounds to the data before segmenting. It seems to be a limitation of the st_bbox and s2_bounds_rect that they give the bounds of the vertices of lines (ie the points), not the line itself. An easy answer, though it increases the size of the dataset, is to move the segmentize to the end of the data preparation pipe. [Check that this works.] You need something like this. Be patient with it. Looks like join takes 50%-100% longer, so we made the right choice. # get the data again routes &lt;- readr::read_csv(&quot;data/Flight_Points_Actual_20190301_20190331.csv.gz&quot;, skip = 1, col_names = c(&quot;id&quot;, &quot;seq&quot;, &quot;time&quot;, &quot;level&quot;, &quot;lat&quot;, &quot;long&quot;)) |&gt; mutate(time = as.POSIXct(time, format = &quot;%d-%m-%Y %H:%M:%S&quot;, tz = &quot;UTC&quot;)) microbenchmark( use_in = routes |&gt; filter(id %in% ac_flts$id), use_join = routes |&gt; inner_join(ac_flts |&gt; select(id), by = &quot;id&quot;), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval ## use_in 790.1362 1045.669 1074.377 1072.778 1139.692 1232.513 10 ## use_join 1311.9271 1398.617 1612.483 1507.648 1740.048 2349.226 10 8.6 Density of routes A rough indication of traffic density can be obtained by plotting the routes with a low transparency (alpha) and seeing where they overlap. This can be visually helpful, but doesn’t give any specific usable values for density. We can get an approximate map of traffic density by picking points along the great circle routes (st_segmentize) at distances of, say, 75km and then counting (making a heatmap, with geom_hex from ggplot2) with a hex grid of the same size. It’s not exact, since some routes might be counted twice in one hex (if they enter and leave through a vertex, say, rather than along the side). But it’s relatively quick to do. Compared to previous maps, the main differences are that we convert the lines into points with st_cast('MULTIPOINT'). This doesn’t add points, it just extracts the vertices of the lines. And we break the rule and transform our data to the target CRS. From the result we extract the coordinates of every point, since this is the sort of data that geom_hex works with. I didn’t find a way to get geom_hex to understand sf data directly, so we break the rule to work in ‘plot coordinates’ (which are x-y in metres when we use crs_europe). We use both the summarise and the purrr method to speed up the calculation (see section 8.5.2), which on this machine is then acceptably quick for a 100,000 flight sample. If there’s a way to pass a count of flights to geom_hex, I haven’t found it, so we use uncount to reverse the count step. sample_km &lt;- 60 flts &lt;- set.seed(777) # to get a fixed, but random sample short_points &lt;- flights |&gt; filter(dist_act_km &lt;= 1500) |&gt; drop_na() |&gt; slice_sample(n = 100000) |&gt; count(adep, ades, adep_long, ades_long, adep_lat, ades_lat, name = &quot;flts&quot;) |&gt; longlat_to_sf() |&gt; #using our faster conversion function st_segmentize(units::set_units(sample_km, km)) bounds &lt;- get_bounds(short_points) # quicker to get bounds now! short_points &lt;- short_points |&gt; st_cast(&#39;MULTIPOINT&#39;) |&gt; st_transform(crs_europe) |&gt; uncount(flts) #duplicate by number of flights just_pts &lt;- st_coordinates(short_points)[ , 1:2] |&gt; as_tibble() ggplot(just_pts) + geom_sf_bg() + geom_hex(aes(X, Y), alpha = 0.7, binwidth = sample_km * 1000) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax))+ theme_minimal() + labs(fill = &quot;Flights\\nin month&quot;, x=&quot;&quot;, y=&quot;&quot;) + scale_fill_viridis_c(direction = -1) To get a more precise density calculation we need to calculate explicitly which routes intersect which hexes. So we need some hexes. There’s a promising package dggridR for doing this, but it’s not available on CRAN at the time of writing, so we use a more fiddly approach that involves some functions in the sp package that we’ve more-or-less avoided using explicitly so far. The steps are: Use the bounding box as the area in which we want hexes. Transform into planar coordinates and convert to a Spatial object Sample points on a hexagonal grid. Convert from points to hexagons. Flip back to sf and then count the intersections. st_intersects lists the ids of hexes intersected, so we can just count. We intersect routes with hexes rather than the reverse because then we can just duplicate hex-ids (for each flight) rather than duplicating routes and doing more intersections. This method is likely to struggle with global maps (because of wrapping problems), so better to focus on a single continent. It generates a couple of warnings (‘discarded datum’, ‘comment ignored’) which can be ignored (they’re not shown in the book). flts &lt;- set.seed(777) # to get a fixed, but random sample n_hex &lt;- 10000 # this gives similar number to the &#39;geom_hex&#39; version. short_routes &lt;- flights |&gt; filter(dist_act_km &lt;= 1500) |&gt; drop_na() |&gt; slice_sample(n = 100000) |&gt; count(adep, ades, adep_long, ades_long, adep_lat, ades_lat, name = &quot;flts&quot;) |&gt; longlat_to_sf() #using our faster conversion function # no segmentation hex &lt;- st_bbox(short_routes) |&gt; st_as_sfc() |&gt; st_transform(crs = crs_europe) |&gt; as_Spatial() |&gt; sp::spsample(n=n_hex, type = &quot;hexagonal&quot;) |&gt; sp::HexPoints2SpatialPolygons() |&gt; st_as_sf() |&gt; st_transform(crs = crs_europe) |&gt; #optional: check areas are all equal # mutate(area = st_area(geometry)) |&gt; rowid_to_column() # add id to each row, for later joining # but not for looping over - oh no! hex_counts &lt;- short_routes |&gt; st_transform(crs = crs_europe) |&gt; st_intersects(hex) |&gt; # duplicate the hex ids, depending on the number of flights map2(short_routes$flts, rep) |&gt; flatten_int() |&gt; #then collapse everything into a single vector as_tibble() |&gt; count(value, name = &quot;flts&quot;) #value is default name in as_tibble # add in the counts to the hexes hex &lt;- hex |&gt; left_join(hex_counts, by = c(&quot;rowid&quot; = &quot;value&quot;)) |&gt; drop_na() # hexes with no traffic bounds &lt;- get_bounds(short_routes) ggplot(hex) + geom_sf_bg() + geom_sf(aes(fill = flts), alpha = 0.7, size = 0) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax))+ theme_minimal() + labs(fill = &quot;Flights\\nin month&quot;, x=&quot;&quot;, y=&quot;&quot;, caption = &quot;Density is based on great-circle routing.&quot;) + scale_fill_viridis_c(direction = -1) Comparing the two version, the former has slightly smaller hexes (it’s difficult to align the numbers precisely since one is defined by diameter and the other by number), but higher flight counts. This emphasises the reality of the overcounting risk: it would be better to drop the numbers from the scale, or use the second method! If you extend to different samples, the process might fail, most likely because the hexes are spreading too wide. The solution is to crop more closely [adapt get_bounds, or the output from it to solve that] or use a hex-on-the-sphere approach such as in dggridR (https://github.com/r-barnes/dggridR/). 8.6.1 Exercises 8.6.1.1 Questions Plot a density map of the AT72 routes, using the accurate intersection method. AT72 are used in passenger, charter and all-cargo markets. Plot a density plot for each of these in separate facets. The facets are easy, but counting by segment as a group variable is quite tricky. If you want to try a simpler question: which four locations see most AT72 use on scheduled services? 8.6.1.2 Answers Assuming that geo_routes is still in your environment, there are minor changes. Mostly because we don’t need to count the flights, we assume that each route is unique. This is a much clearer picture of where the flights are most common. # assume you still have geo_routes in your environment n_hex &lt;- 10000 hex &lt;- st_bbox(geo_routes) |&gt; # just this line changes st_as_sfc() |&gt; st_transform(crs = crs_europe) |&gt; as_Spatial() |&gt; sp::spsample(n=n_hex, type = &quot;hexagonal&quot;) |&gt; sp::HexPoints2SpatialPolygons() |&gt; st_as_sf() |&gt; st_transform(crs = crs_europe) |&gt; rowid_to_column() hex_counts &lt;- geo_routes |&gt; st_transform(crs = crs_europe) |&gt; st_intersects(hex) |&gt; # duplicate the hex ids, depending on the number of flights flatten_int() |&gt; #then collapse everything into a single vector as_tibble() |&gt; count(value, name = &quot;flts&quot;) #value is default name in as_tibble # add in the counts to the hexes hex &lt;- hex |&gt; left_join(hex_counts, by = c(&quot;rowid&quot; = &quot;value&quot;)) |&gt; drop_na() # hexes with no traffic bounds &lt;- get_bounds(geo_routes) ggplot(hex |&gt; mutate(group = sample(1:2, n(), replace = TRUE))) + geom_sf_bg() + geom_sf(aes(fill = flts), alpha = 0.7, size = 0) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax))+ theme_minimal() + labs(title = &quot;ATR72 routes&quot;, fill = &quot;Flights\\nin month&quot;, x=&quot;&quot;, y=&quot;&quot;, caption = &quot;Density is based on actual routing.&quot;) + scale_fill_viridis_c(direction = -1) This is a difficult one. We add the segments back into geo_routes, and pull out that column stand-alone. We hang on to st_intersects which is quick and is a sort of list so we use it as a tibble column. Then bind on the segments column, which gives our groups. At this point, we want one list or vector of hex-ids per segment so use summarise to collapse into a vector wrapped in a list so that it can also be a list column. And that can simply be unnested, ready for counting. I’m happy to believe you might find an easier way to do this. markets &lt;- geo_routes |&gt; # let&#39;s keep all segments for now left_join(ac_flts |&gt; select(id, segment)) |&gt; pull(segment) ## Joining, by = &quot;id&quot; # the hexes can be the same as in Qn 1 n_hex &lt;- 10000 hex &lt;- st_bbox(geo_routes) |&gt; # just this line changes st_as_sfc() |&gt; st_transform(crs = crs_europe) |&gt; as_Spatial() |&gt; sp::spsample(n=n_hex, type = &quot;hexagonal&quot;) |&gt; sp::HexPoints2SpatialPolygons() |&gt; st_as_sf() |&gt; st_transform(crs = crs_europe) |&gt; rowid_to_column() hex_counts &lt;- tibble( hex_id = st_intersects(geo_routes |&gt; st_transform(crs = crs_europe), hex), market = markets) |&gt; group_by(market) |&gt; summarise(hex_id = list(flatten_int(hex_id))) |&gt; unnest(hex_id) |&gt; count(market, hex_id, name = &quot;flts&quot;) # add in the counts to the hexes hex_m &lt;- hex |&gt; left_join(hex_counts, by = c(&quot;rowid&quot; = &quot;hex_id&quot;)) |&gt; drop_na() # hexes with no traffic bounds &lt;- get_bounds(geo_routes) ggplot(hex_m) + geom_sf_bg() + geom_sf(aes(fill = flts), alpha = 0.7, size = 0) + facet_wrap(~market) + coord_sf(crs = crs_europe, xlim = c(bounds$xmin, bounds$xmax), ylim = c(bounds$ymin, bounds$ymax))+ theme_minimal() + labs(title = &quot;ATR72 routes&quot;, fill = &quot;Flights\\nin month&quot;, x=&quot;&quot;, y=&quot;&quot;, caption = &quot;Density is based on actual routing.&quot;) + scale_fill_viridis_c(direction = -1) 8.7 What has gone wrong? With maps it seems that there are just so many things to get wrong. Map issues, especially polygons that seem to have crossings in them; errors like Loop 96 is not valid: Edge 743 crosses edge 998. This could be that there is a problem in the map, but it can also be an awkward interaction between the map and a CRS especially at the ‘dateline’ (the ‘far’ edge of the map). There isn’t a single solution to this, but filtering out chunks of the map that you don’t need can work. In other cases, simplifying the map might help (st_simplify), in still others adding points can help(st_segmentize). Empty maps, or maps with large empty areas. A map of Europe squeezed into the top right corner can mean that you’ve added a ggplot layer that is in long-lat to one that has been projected and is in metres (so that the long-lat get plotted somewhere near 0 long, 0 lat). You might also have a map that is being strict about including French Guiana and other overseas territories. In the first case, see the coord_sf() discussion earlier in this chapter. In the latter case, you’ll need to set bounds for the map (see the later part of section 8.3.1). In World maps, unexpected near-horizontal lines is a sign of cropping and dateline troubles. My usual solution is to use himach::st_window, which is basically a replacement for the st_transform that we’ve seen here that first cropps to the final view window. It’s designed for use with the crs_Atlantic and crs_Pacific CRSs that come with the himach package. There are other functions for handling the ‘dateline’, but I haven’t found an easier way to handle all types of geometry (line, polygon etc) so reliably. map &lt;- rnaturalearthdata::coastline110 |&gt; st_as_sf() # a Pacific view creates problems along Greenwich meridian ggplot(map) + geom_sf() + labs(title = &quot;A map with wrapping problems at the &#39;far side&#39;.&quot;) + coord_sf(crs = himach::crs_Pacific) # st_window ggplot(map |&gt; himach::st_window(himach::crs_Pacific))+ labs(title = &quot;Wrapping problems solved.&quot;) + geom_sf() Strictly speaking, when we say ‘great circle’ in this book we really mean ‘segment of a great circle’, or ‘along a great circle’↩︎ "],["splitparse.html", "Chapter 9 NOTAMS: Splitting and parsing strings 9.1 Splitting a column into many 9.2 List columns", " Chapter 9 NOTAMS: Splitting and parsing strings NOTE: This chapter has been written out of sequence, so jumps some steps and may feel a bit of a leap from the (current) previous chapter. Need to add in between chapters on: dates, mutate-across, lapply, named list (extracting from) TBD. This chapter uses the example of NOTAMs (notices to airmen) to explore R functions for turning text into more useable data, by splitting and parsing. The parsing of NOTAMs here is NOT for operational use. It’s for statistical analysis, post operations. NOTAMs are very varied, and we will not handle all the possible ways in which the parsing might fail. We assume that failures can be picked up in later statistical analysis, or will just be outliers that can be ignored as 1 in millions of cases. This code is NOT A SUBSTITUTE FOR READING THE NOTAM BEFORE THE FLIGHT. In this chapter, you’ll be introduced to: separate(), regular expressions, list fields, system.time(), identical(), str_match, str_split(), str_replace(_all)(), str_sub(), unnest_(wider/longer)(), map2(), relocate The sections are ordered not in the order you’d have to work, starting with a full NOTAM text, but in a rough order of simple to more complex. We assume throughout that you have not just a single string, but a dataframe of strings for processing. This is because we want to illustrate this way of working: ‘vectorised’ in the R jargon. You could write a function that takes a single string and parses it. Then call it many times and stack the results together with rbind or bind_rows. But we’re planning to run this millions of times, so that’s millions of function calls. So instead we write in a way that works on the whole dataset at once, and leaves the tidyverse to do that in the most efficient way. (There’s an illustration of the time penalty later in the chapter.) 9.1 Splitting a column into many Within the tidyverse tidyr provides some useful tools for parsing text. In this chapter we see a simpler and a more complex use of tidyr::separate. 9.1.1 Splitting a column at a character In the simplest case, a string is to be separated at a single separator character. In field Q of the NOTAM this is “/”. We just need to tell separate what the new columns are called, as in the following code. library(tidyverse) # a couple of examples, in a dataframe q_fields &lt;- data.frame(q = c(&quot;GLRB/QPLXX/IV/NBO/E /000/999/0620N01206W483&quot;, &quot;LYBA/QKKKK/K /K /K /000/999/4234N02102E999&quot;) ) q_parsed &lt;- q_fields %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) #pretty view for the book knitr::kable(q_parsed) FIR qgroup IV NBO AEW FL_lo FL_hi geo GLRB QPLXX IV NBO E 000 999 0620N01206W483 LYBA QKKKK K K K 000 999 4234N02102E999 There are some fields here which could be parsed further (qgroup and geo), but we leave that. I suspect that the ‘K’ is a missing value, which we could translate into an R NA value, but we don’t go that far here. We will assume that all the q-fields are properly formed, of these 8 fields. A more robust piece of code would check this is true. This is for analysis, not operations - you’re getting the message I think. Inspecting the raw strings (always essential!) shows that there are some blank spaces. So in our result we have some fields with trailing spaces. We could use stringr::str_trim on the results to get rid of trailing spaces, and indeed, that would probably be a good idea anyway. But we can use this to illustrate the next level in use of separate, which we do in the next section. 9.1.2 Splitting a column at a regular expression The sep= is a ‘regular expression’. Entire books have been written about regular expressions, so my advice would be to find a page you like, or download a cheatsheet of which there are many. This is the one I use, being R-focused. A regular expression is a search or matching pattern. We want to split the q-field at not just \\ but any number of spaces followed by a \\. That will remove the trailing spaces, because they’ll be treated as part of the separator. Our string has some ‘newlines’ in it \\n as well as spaces, and in other applications perhaps tabs are used. So we use a \\\\s code, rather than just a space character ” “. \\\\s stands for all of these types of ‘space’, and some others [check in the cheatsheet]. In regular expression terms this is \\\\s*/, which you can read as zero-or-more spaces followed by a back slash. We can show that this gives the same result as using trim. # what you&#39;d get using trim q_parsed_trim &lt;- q_fields %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) %&gt;% mutate(across(.fns = str_trim)) #what you get with the search pattern q_parsed_regex &lt;- q_fields %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), # separate on any number of spaces followed by a backslash sep = &quot;\\\\s*/&quot;) # are they the same? identical(q_parsed_trim, q_parsed_regex) ## [1] TRUE They are identical. Which do you prefer? The second involves one less line of code, but you might feel the need to add a comment, as I’ve done here, because the code is more obscure. So it’s the same number of lines to read. That’s a qualitative judgement. If you wanted to run this thousands of times, maybe you would run timing tests. Something like this. q_many &lt;- data.frame(q = rep(q_fields$q, 100000)) # what you&#39;d get using trim system.time( q_parsed_trim &lt;- q_many %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) %&gt;% mutate(across(.fns = str_trim)) ) ## user system elapsed ## 4.094 0.777 4.990 #what you get with the search pattern system.time( q_parsed_regex &lt;- q_many %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), # separate on any number of spaces followed by a backslash sep = &quot;\\\\s*/&quot;) ) ## user system elapsed ## 3.779 0.705 4.602 # and for fun, what does calling the function many times look like? # just do 1/100th of the rows system.time( z &lt;- lapply(q_many$q[1:1000], function(x) data.frame(q=x) %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;\\\\s*/&quot;)) ) ## user system elapsed ## 1.941 0.042 2.029 # are they the same? identical(q_parsed_trim, q_parsed_regex) ## [1] TRUE On my machine, the first takes about 40% longer in elapsed time, and twice the system time. So having fewer lines of code is more efficient in this case. In a light-heared way, we also show a timing test for calling the function each time. Yes, the code that isn’t carefully designed for speed, but for a dataset 100 times smaller already takes longer than either of the other two. Letting R do its vectorised thing is indeed better! 9.1.3 Splitting a column at an either-or Taking this one step further, we can also work on the NOTAM header. This has NOTAM in it, which isn’t that useful information, so we can ditch in, but it has this in the form ‘NOTAMC’, ‘NOTAMR’, ‘NOTAMN’ depending on whether this is cancelling, replacing or a new NOTAM, respectively. The structure of the field varies between these cases. As always, there are several ways to approach this. We could first remove the ‘NOTAM’ then split at spaces. But that’s two steps. So instead, we split at either ’ NOTAM’ or ’ ’ , in one go. In regular expressions, either or is given by |, just as in R syntax for ‘or’. And to be on the safe side, we use the more general code for a ‘space’, though a new line is unlikely in this position. header_fields &lt;- data.frame(header = c( &quot;A0001/13 NOTAMN &quot;, &quot;A0001/13 NOTAMR A0032/12&quot;, &quot;W0809/13 NOTAMC W0808/13&quot; )) header_parsed &lt;- header_fields %&gt;% separate(header, c(&quot;notam&quot;, &quot;type&quot;, &quot;replaces&quot;), sep = &quot;\\\\sNOTAM|\\\\s&quot;) #pretty view for the book knitr::kable(header_parsed) notam type replaces A0001/13 N A0001/13 R A0032/12 W0809/13 C W0808/13 This works neatly, partly because there’s a trailing space at the end of the first one. [In the code, delete the space and re-run. What happens? How does the header-parsed dataframe change?] In bulk use, you could either live with the Expected 3 pieces.... errors, or add an extra space. Probably this is a sign that we should look for a better way to split, but this will always be tricky when the number of parts varies from one to the next, as it does here. 9.1.4 Exercises 9.1.4.1 Questions If \\\\s matches a space, what matches everything that is not a space? How did the header_parsed dataframe change when you removed the space? Look up the format of q_parsed$geo on the internet. Split it into 3 parts of fixed length, not based on a separator, but appropriately to the meaning. 9.1.4.2 Answers \\\\S, from the cheat sheet. There’s also a more complex version of the same thing [^[:space]]. One value changed from empty string, ““, to NA. The help file explains that sep can also be positions. Try separate(geo, c(\"Lat\",\"Long\",\"Radius\"), c(5, 11)). 9.2 List columns In the last section we saw various ways to split one column into several. Here we use another approach. Conceptually, it’s harder, but it keep the parts ‘similar’ for longer, so we can modify them further en masse rather than one column at a time. As usual this is not the only way this could be done, but it’s a good example of the use of list columns. List columns? Up to now, you might have been thinking of tidy datasets like a table in a spreadsheet. Not a bad analogy, but R has more tricks up its sleeve. One particularly powerful one is that any column can contain, not just single values, but lists. There’s a whole package in tidyverse called purrr (yes, 3 ’r’s) to help handle such things. List columns are great for modelling. You can have rows with just 3 cells, say, one a country name, the next the whole historic time series, the next the forecast. Extremely compact. The application here is simpler. The pattern we’ll introduce here has 4 steps: Split a complex text column into a list (one column) Flip that list to generate extra rows in the dataset Manipulate those rows, in groups Pivot to generate 1 row per text, again, and multiple columns. Why might this be easier than approaches using separate? Because we want to manipulate what would be the column names after separate, it’s easier to do if these are instead text fields. 9.2.1 Splitting a field into a list We load a set of NOTAM texts from Github. These are based on real ones, but some have been manipulated to create parsing challenges that have been seen in larger sets. The problems are in the later ones, so we will use short samples to begin with: start with a small slice is a good principle to follow anyway. (see TBD handling large datasets) Sites like Eurocontrol’s EAD Basic provide views onto ready-parsed NOTAM data. So this chapter is relevant either (a) to learn string manipulation with regular expressions or (b) if you happen to receive bulk data for analysis, or both. When adding rows, as we will, we need an index that lets us identify all of the rows that go with one piece of text. We generate a simple integer index for this. The initial processing is to remove the opening and closing bracket from the whole NOTAM. This is just for sake of tidiness, though once we split the field the “()” are no longer a matched pair, so best to do it now. [How does the regular expression ^\\\\(|\\\\)$ achieve this?] NOTAMs are in sections, with headers “Q)”, “A)”, “B)” to “G)”. So, in a first attempt (which works in very simple cases only), we want to split where we find ‘section’ headers “Q)”, “A)” etc. But we don’t want to throw these away, we want to split and keep the separator. The regular expression is \\\\s+[QABCDEFG]\\\\)\\\\s to match any of the allowable headers in a NOTAM. You’ve seen most of these parts, the only new one is [QABCDEFG], which says any one of these characters (case-sensitive). The extra bit is (?= ) which says: and leave behind the value corresponding to the part after the =, after splitting on its position. We choose to forget any leading spaces, so the \\\\s+ is before this bracketed term. # get the NOTAM texts and add an ID notam_url &lt;- &quot;https://github.com/david6marsh/flights_in_R/raw/main/data/NOTAMSample.csv&quot; notam_texts &lt;- read.csv(notam_url) %&gt;% mutate(index = row_number()) notams &lt;- notam_texts %&gt;% slice(1:2) %&gt;% #just a few to start with mutate( # remove start and end brackets full = stringr::str_replace_all(NOTAM, &quot;^\\\\(|\\\\)$&quot;, &quot;&quot;), # split out the phrases based on Q) A) etc phrases = stringr::str_split(full, &quot;\\\\s+(?=[QABCDEFG]\\\\)\\\\s)&quot;)) # quick look at one list. notams$phrases[1] ## [[1]] ## [1] &quot;A0011/10 NOTAMN&quot; ## [2] &quot;Q) PAZA/QMRXX/IV/NBO/A /000/999/6034N15115W005&quot; ## [3] &quot;A) PAEN&quot; ## [4] &quot;B) 1001011230&quot; ## [5] &quot;C) 1001031230 EST&quot; ## [6] &quot;E) RWY 01L/19R PATCHY THIN ICE SANDED&quot; That’s what a single list element looks like when you print it to the log. Have a look in the Environment pane to see what a list column looks like. It should like this, depending on how wide your window is! The dataframe has 2 observations. So phrases is shown as a “List of 2”. It has to be the same length as the number of rows in the dataframe. Each element in phrases is also a list, so it’s a list of lists, but each element can be of any length. [Check by inspection, or run lapply(notams$phrases, length).] 9.2.2 More realistic splitting In real NOTAMs, “A)” can be a header as we saw above, but it can also be a bullet for a bulleted list within field “E)”. After trying a number of approaches to parsing this, I’ve opted here to split the NOTAM into 2 parts at the first “E)” and handle them separately. In fact it allows me to illustrate some more features of list fields. If there were only one “E)” we could just use separate to do this. But “E)” can also appear as a bullet, so we take a multi-step approach: split at “E)” using str_split because we can insist on only have 2 parts, ie splitting at the first occurrence; add names to the resulting list column, using setNames; then unnest_wider, ie into columns. If this isn’t an obvious approach, don’t worry, because it wasn’t obvious to me either. It took trial and error, and a bit of googling to find this combination. The error was finding the hard way that unnest_wider won’t let you provide names for the new columns (as we saw above that separate does), it insists on finding them in the data. [You can see the ‘errors’ by commenting out the line with map in it.] The googling was how to provide names for unnest_wider. As usual, someone has already struggled with this, and an answer was out there. notams &lt;- notam_texts %&gt;% slice(1:7) %&gt;% #all but the last one mutate( # remove start and end brackets full = stringr::str_replace_all(NOTAM, &quot;^\\\\(|\\\\)$&quot;, &quot;&quot;), # split into two parts split_at_E = stringr::str_split(full, &quot;\\\\s(?=E\\\\)\\\\s)&quot;, n=2), split_at_E = map(split_at_E, setNames, c(&quot;part1&quot;, &quot;part2&quot;)) ) %&gt;% unnest_wider(split_at_E) We can now split out the fields from the first part as in our first attempt, splitting at \\\\s+(?=[QABCD]\\\\)\\\\s). The second part is still tricky, because “F)” and “G)” can be fields or bullets in a bulleted list. The two fields are rare, and describe the vertical extent of the NOTAM. We find the ‘real’ fields by looking for something that resembles a vertical extent. Inspection of many cases shows we have values such as: 1200FT AMSL (above mean sea level), or that could be in metres ‘M’ or above ground level (AGL) or surface SFC. There might or might not be a space between. [What would the regular expression look like?] FL310 (flight level) or just SFC or GND (ground) for the bottom (field F) or UNL (unlimited) in either field. Put all of these together and we get an expression like ffg in the next code chunk; we could have found slightly different expressions for F and G, but this single one works for both, so it’s good enough for us here. Use the cheatsheet for terms that may be new (eg \\\\d). It can be easier to read if you look for () pairs, starting with ones close together, and for the | symbol. [Which expression allows for a possible but not necessary space? Find each the listed items above, separated by |.] Having split, and generated two list fields, we need to recombine them. Normally you’d join list with c(), eg c(list(1), list(7,5)). Here we do that too, but within the mutate function have to write it in a longer form, using map2 (TBD - explain reasons?). Then, finally, we get to step (2) of our pattern (remember that?) and un-nest into rows. ffg &lt;- &quot;(?=[FG]\\\\)\\\\s*((\\\\s*\\\\d+(FT|M)\\\\s*(AGL|AMSL|SFC))|(FL\\\\d+)|SFC|GND|UNL))&quot; # before the unnest, how many rows do we have nrow(notams) ## [1] 7 notams &lt;- notams %&gt;% #split these - note force the first to be a list column mutate(part1 = str_split(part1, &quot;\\\\s+(?=[QABCD]\\\\)\\\\s)&quot;), part2 = str_split(part2, ffg), both = purrr::map2(part1, part2, c)) %&gt;% select(!c(part1, part2)) %&gt;% unnest_longer(both) # show that we&#39;ve added quite a few rows nrow(notams) ## [1] 45 [Check visually that in NOTAM 4, the fields F and G have been pulled out but that in NOTAM 6 bullets F and G are left within field E.] If we had used separate in place of str_split then we would generate missing fields NA, and potentially (if there were a D field but no C, say) map data into the wrong field. [Exercise - try with separate] 9.2.3 Manipulate fields in rows Step 3 is to generate what will become the column names. After all those regular expressions, we’ll stick to some basic extraction of parts of strings for this. We want the first element to be called header. So we use groups as we saw earlier (TBD), and row_number to get the first item. Then converting to columns is done with pivot_wider. [Why not unnest? Because we not dealing with list columns now, but ‘ordinary’ ones.] The order in which this happens is not quite perfect, so we relocate it, even if it will make no difference to the analysis, it will help us when scrolling back and forth in the data viewer. notams &lt;- notams %&gt;% group_by(index) %&gt;% # sort out the names mutate(id = if_else(row_number()==1, &quot;header&quot;, str_sub(both, 1, 1)), # having taken field id, remove the id from the text both = if_else(id == &quot;header&quot;, both, str_sub(both, 4, nchar(both)))) %&gt;% # then convert to columns pivot_wider(names_from = id, values_from = both, names_prefix = &quot;field_&quot;) %&gt;% ungroup() %&gt;% # no longer need this index and NOTAM is a near-duplicate of full select(!c(index, NOTAM)) %&gt;% # put D in its place relocate(field_D, .before = field_E) That’s steps 3 and 4 done; we’ve finished extracting the fields from the raw text. We can move on to parsing the fields, to pull out information from then. In fact, we already did this in section 9.1.1. You could also convert text to dates. Pulling forward the code from those earlier sections we get something like this. notams &lt;- notams %&gt;% mutate( from_date = as.POSIXct(field_B, format = &quot;%y%m%d%H%M&quot;, tz = &quot;UTC&quot;), #for C we will get errors but that&#39;s OK to_date = as.POSIXct(field_C, format = &quot;%y%m%d%H%M&quot;, tz = &quot;UTC&quot;)) %&gt;% # split some fields # this warns on generating NAs, which is what we want separate(field_header, c(&quot;notam&quot;, &quot;type&quot;, &quot;replaces&quot;), sep = &quot;( NOTAM| )&quot;) %&gt;% separate(field_Q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) %&gt;% # put them in a helpful order relocate(from_date, .after = field_B) %&gt;% relocate(to_date, .after = field_C) %&gt;% relocate(full, .after = last_col()) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 5 rows [1, 4, 5, ## 6, 7]. That’s as far as we need to go. The code generates some warnings of ‘missing pieces’, but otherwise works on these examples. The exercises explore a bit further, including how this ‘basic’ set of code might fail for some NOTAMs. 9.2.4 Exercises 9.2.4.1 Questions How does the regular expression ^\\\\(|\\\\)$ achieve what we need? What would str_replace have done differently to str_replace_all? stringr::str_replace_all would also accept a vector of regular expressions. Predict what using c(\"^\\\\(\",\"\\\\)$\") would deliver. Then try it out. [Hint: there are 2 rows.] In \\\\s+(?=[QABCDEFG]\\\\)\\\\s), which closing bracket goes with the opening bracket? Create a dataframe with a named list column, for which each entry is a named list. [Hint: I find this surprisingly fiddly. Create a 2-row, 1 field dataframe. Then add the named list column.] How can you pull out a single value from this? Use str_match to pull the A field directly from the NOTAM field in notam_texts. Try two approaches, one assuming that the string ends in “B)” and another based only on the likely content of the field. Create a function that takes a dataframe like ‘notam_texts’ and returns the result of the whole process. What happens if you run it on the last NOTAM in the file and why? In words, how might you solve this? In words, how might you solve the ‘missing pieces’ warnings? Where would our code struggle if a NOTAM has repeated fields? (I’ve seen this occur: two different (!) versions of fields F and G)? 9.2.4.2 Answers The cheat sheet will tell you: ^ matches at the start of the string, $ at the end. ( and ) are reserved tokens in regular expressions, so you need to ‘escape’ them to use them as literal characters. | we saw earlier, means ‘or’. It just changes the first occurrence. Compare str_replace(q_fields$q, \"/\", \"-$-\") and str_replace_all(q_fields$q, \"/\", \"-$-\"). The usual R behaviour is to cycle through the values. So it alternates, using the first for the odd rows (ie the first one) and the second for the even rows. So the first loses the starting ( and the second loses just the ending ). The first closing bracket is preceded by \\\\ so it is treated as a literal character, not part of the code. By elimination, then, it has to be the second one. For example z &lt;- data.frame(id = 1:2) z$named &lt;- list(a=c(x=1, y=2), b=c(z=4)) With that example, you can use z$named$a[\"y\"], for example. Amongst many possible solutions, these work. How would you select A if it contains multiple 3 or 4 letter strings separated by spaces? ans &lt;- notam_texts %&gt;% mutate(v1 = str_match(NOTAM, &quot;A\\\\)\\\\s+[^(B\\\\))]+&quot;), v2 = str_match(NOTAM, &quot;A\\\\)\\\\s+[[:upper:]]{4}&quot;), # multiply one field, for testing notam_plus = str_replace(NOTAM, &quot;RPXX&quot;, &quot;RPXX EGLL&quot;), vMult = str_match(notam_plus, &quot;A\\\\)(\\\\s+[[:upper:]]{4})+&quot;)) (Essentially a cut-paste exercise, so result not provided here.) The final NOTAM has a missing “E)” header for the E field, so the splitting approach fails. Two possible solutions: filter to remove, if “E)” not found would perhaps be ok as one outlier amongst thousands; or detect and insert “E)” after C or D to allow the code to work properly. Perhaps there’s a ‘quiet’ option for the function ;-). Otherwise, since the problem is whether or not there’s text to say which NOTAM is being replaced, you could test for length and separate in two different ways: if_else(header is long, separate with 3 column names, separate with 2 column names). A hard one, but this leads pivot_wider() to create a list column rather than an ordinary one. A nice robust response, but possibly not what you want. In practice, I used the additional parameters values_from = both, values_fn = gdata::first to handle this and arbitrarily pick the first of the duplicate entries. This might not suit your need. "],["glossary.html", "Chapter 10 Glossary", " Chapter 10 Glossary Aviation is chock-a-block with acronyms. A good place to look for these is in AIRIAL. The language of aviation and aviation statistics is also quite precise, though in practice people might say ‘flight’ when they strictly mean ‘flight segment’. There is an elderly EUROCONTROL glossary of statistics and a more recent Eurostat one. This glossary chapter is deliberately chooses more hand-waving definitions, over the strictly water-tight. Open Data We use this in a broad sense, to include data that is available for free for non-commercial use. It might not be free for onward distribution. You might need to ask for a logon to get access to it. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
